{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">#01 | Why Neural Networks Deeply Learn a Mathematical Formula</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subscribe to my [Blog ‚Üó](https://blog.pythonassembly.com/)\n",
    "- Let's keep in touch on [LinkedIn ‚Üó](www.linkedin.com/in/jsulopz) üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discipline to Search Solutions in Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Apply the following steps when **looking for solutions in Google**:\n",
    ">\n",
    "> 1. **Necesity**: How to load an Excel in Python?\n",
    "> 2. **Search in Google**: by keywords\n",
    ">   - `load excel python`\n",
    ">   - ~~how to load excel in python~~\n",
    "> 3. **Solution**: What's the `function()` that loads an Excel in Python?\n",
    ">   - A Function to Programming is what the Atom to Phisics.\n",
    ">   - Every time you want to do something in programming\n",
    ">   - **You will need a `function()`** to make it\n",
    ">   - Theferore, you must **detect parenthesis `()`**\n",
    ">   - Out of all the words that you see in a website\n",
    ">   - Because they indicate the presence of a `function()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning, what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The Machine Learns...\n",
    ">\n",
    "> But, **what does it learn?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Machine Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Practical Example ‚Üí [Tesla Autopilot](https://www.tesla.com/AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Example where It Fails ‚Üí [Tesla Confuses Moon with Semaphore](https://twitter.com/Carnage4Life/status/1418920100086784000?s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Simply execute the following lines of code to load the data.\n",
    "> - This dataset contains **statistics about Car Accidents** (columns)\n",
    "> - In each one of **USA States** (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/fivethirtyeight/fivethirtyeight-bad-drivers-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CT</th>\n",
       "      <td>10.8</td>\n",
       "      <td>4.968</td>\n",
       "      <td>3.888</td>\n",
       "      <td>9.396</td>\n",
       "      <td>8.856</td>\n",
       "      <td>1068.73</td>\n",
       "      <td>167.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VA</th>\n",
       "      <td>12.7</td>\n",
       "      <td>2.413</td>\n",
       "      <td>3.429</td>\n",
       "      <td>11.049</td>\n",
       "      <td>11.176</td>\n",
       "      <td>768.95</td>\n",
       "      <td>153.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NY</th>\n",
       "      <td>12.3</td>\n",
       "      <td>3.936</td>\n",
       "      <td>3.567</td>\n",
       "      <td>10.824</td>\n",
       "      <td>9.840</td>\n",
       "      <td>1234.31</td>\n",
       "      <td>150.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>15.6</td>\n",
       "      <td>2.964</td>\n",
       "      <td>3.900</td>\n",
       "      <td>14.820</td>\n",
       "      <td>14.508</td>\n",
       "      <td>913.15</td>\n",
       "      <td>142.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>16.2</td>\n",
       "      <td>6.156</td>\n",
       "      <td>4.860</td>\n",
       "      <td>14.094</td>\n",
       "      <td>16.038</td>\n",
       "      <td>1137.87</td>\n",
       "      <td>151.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "CT       10.8     4.968    3.888           9.396        8.856      1068.73   \n",
       "VA       12.7     2.413    3.429          11.049       11.176       768.95   \n",
       "NY       12.3     3.936    3.567          10.824        9.840      1234.31   \n",
       "GA       15.6     2.964    3.900          14.820       14.508       913.15   \n",
       "DE       16.2     6.156    4.860          14.094       16.038      1137.87   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "CT          167.02  \n",
       "VA          153.72  \n",
       "NY          150.01  \n",
       "GA          142.80  \n",
       "DE          151.48  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset(name='car_crashes', index_col='abbrev')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Neural Network Model in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=6, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the `weights` to start in 0\n",
    "\n",
    "> - `kernel_initializer='zeros'` on second `Dense` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=6, activation='relu', kernel_initializer='zeros'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_15749/1548584363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "AL       18.8     7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 12:31:54.238525: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for All USA States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>initial_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  initial_pred\n",
       "abbrev                     \n",
       "AL       18.8           0.5\n",
       "AK       18.1           0.5\n",
       "AZ       18.6           0.5\n",
       "AR       22.4           0.5\n",
       "CA       12.0           0.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['initial_pred'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - [ ] Why are these predictions so far away from reality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Mathematical Equation of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In other words: calculate the `best numbers` for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_15749/897466048.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;31m# Legacy graph support is contained in `training_v1.Model`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0mversion_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisallow_legacy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2788\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2790\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2791\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 12:33:12.161541: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 65ms/step - loss: 0.6872 - accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6572 - accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6262 - accuracy: 0.0000e+00\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.0000e+00\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5651 - accuracy: 0.0000e+00\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5344 - accuracy: 0.0000e+00\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5037 - accuracy: 0.0000e+00\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.0000e+00\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4427 - accuracy: 0.0000e+00\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4125 - accuracy: 0.0000e+00\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3819 - accuracy: 0.0000e+00\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3512 - accuracy: 0.0000e+00\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3205 - accuracy: 0.0000e+00\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2903 - accuracy: 0.0000e+00\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2600 - accuracy: 0.0000e+00\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2291 - accuracy: 0.0000e+00\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1989 - accuracy: 0.0000e+00\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1680 - accuracy: 0.0000e+00\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1375 - accuracy: 0.0000e+00\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.0000e+00\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.0000e+00\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0461 - accuracy: 0.0000e+00\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0154 - accuracy: 0.0000e+00\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.0150 - accuracy: 0.0000e+00\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -0.0452 - accuracy: 0.0000e+00\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.0762 - accuracy: 0.0000e+00\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.1068 - accuracy: 0.0000e+00\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -0.1373 - accuracy: 0.0000e+00\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.1677 - accuracy: 0.0000e+00\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.1983 - accuracy: 0.0000e+00\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: -0.2286 - accuracy: 0.0000e+00\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: -0.2594 - accuracy: 0.0000e+00\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -0.2899 - accuracy: 0.0000e+00\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.3201 - accuracy: 0.0000e+00\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: -0.3508 - accuracy: 0.0000e+00\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: -0.3812 - accuracy: 0.0000e+00\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.4115 - accuracy: 0.0000e+00\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -0.4422 - accuracy: 0.0000e+00\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.4727 - accuracy: 0.0000e+00\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.5030 - accuracy: 0.0000e+00\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.5334 - accuracy: 0.0000e+00\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.5643 - accuracy: 0.0000e+00\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.5945 - accuracy: 0.0000e+00\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.6251 - accuracy: 0.0000e+00\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.6559 - accuracy: 0.0000e+00\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.6856 - accuracy: 0.0000e+00\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.7168 - accuracy: 0.0000e+00\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.7470 - accuracy: 0.0000e+00\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.7771 - accuracy: 0.0000e+00\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.8079 - accuracy: 0.0000e+00\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.8387 - accuracy: 0.0000e+00\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.8687 - accuracy: 0.0000e+00\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.8988 - accuracy: 0.0000e+00\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -0.9290 - accuracy: 0.0000e+00\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.9599 - accuracy: 0.0000e+00\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -0.9902 - accuracy: 0.0000e+00\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.0205 - accuracy: 0.0000e+00\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.0507 - accuracy: 0.0000e+00\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -1.0813 - accuracy: 0.0000e+00\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -1.1116 - accuracy: 0.0000e+00\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.1416 - accuracy: 0.0000e+00\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.1725 - accuracy: 0.0000e+00\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.2029 - accuracy: 0.0000e+00\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.2335 - accuracy: 0.0000e+00\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.2637 - accuracy: 0.0000e+00\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.2947 - accuracy: 0.0000e+00\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.3245 - accuracy: 0.0000e+00\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.3553 - accuracy: 0.0000e+00\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -1.3860 - accuracy: 0.0000e+00\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.4163 - accuracy: 0.0000e+00\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -1.4468 - accuracy: 0.0000e+00\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.4774 - accuracy: 0.0000e+00\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.5078 - accuracy: 0.0000e+00\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.5382 - accuracy: 0.0000e+00\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -1.5688 - accuracy: 0.0000e+00\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.5995 - accuracy: 0.0000e+00\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.6297 - accuracy: 0.0000e+00\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.6600 - accuracy: 0.0000e+00\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.6906 - accuracy: 0.0000e+00\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.7214 - accuracy: 0.0000e+00\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - ETA: 0s - loss: -1.7634 - accuracy: 0.0000e+0 - 0s 6ms/step - loss: -1.7515 - accuracy: 0.0000e+00\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.7824 - accuracy: 0.0000e+00\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.8121 - accuracy: 0.0000e+00\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.8430 - accuracy: 0.0000e+00\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.8730 - accuracy: 0.0000e+00\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -1.9036 - accuracy: 0.0000e+00\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -1.9345 - accuracy: 0.0000e+00\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -1.9644 - accuracy: 0.0000e+00\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -1.9950 - accuracy: 0.0000e+00\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -2.0254 - accuracy: 0.0000e+00\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -2.0556 - accuracy: 0.0000e+00\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -2.0855 - accuracy: 0.0000e+00\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -2.1161 - accuracy: 0.0000e+00\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.1467 - accuracy: 0.0000e+00\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.1772 - accuracy: 0.0000e+00\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.2073 - accuracy: 0.0000e+00\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.2378 - accuracy: 0.0000e+00\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.2678 - accuracy: 0.0000e+00\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.2986 - accuracy: 0.0000e+00\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -2.3292 - accuracy: 0.0000e+00\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.3594 - accuracy: 0.0000e+00\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.3891 - accuracy: 0.0000e+00\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.4203 - accuracy: 0.0000e+00\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.4505 - accuracy: 0.0000e+00\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.4812 - accuracy: 0.0000e+00\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.5117 - accuracy: 0.0000e+00\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.5419 - accuracy: 0.0000e+00\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.5721 - accuracy: 0.0000e+00\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.6026 - accuracy: 0.0000e+00\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.6335 - accuracy: 0.0000e+00\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -2.6636 - accuracy: 0.0000e+00\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.6937 - accuracy: 0.0000e+00\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.7249 - accuracy: 0.0000e+00\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.7548 - accuracy: 0.0000e+00\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.7852 - accuracy: 0.0000e+00\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.8159 - accuracy: 0.0000e+00\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.8463 - accuracy: 0.0000e+00\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.8766 - accuracy: 0.0000e+00\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.9068 - accuracy: 0.0000e+00\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: -2.9375 - accuracy: 0.0000e+00\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -2.9677 - accuracy: 0.0000e+00\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -2.9981 - accuracy: 0.0000e+00\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.0288 - accuracy: 0.0000e+00\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.0593 - accuracy: 0.0000e+00\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.0897 - accuracy: 0.0000e+00\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.1198 - accuracy: 0.0000e+00\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.1503 - accuracy: 0.0000e+00\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.1810 - accuracy: 0.0000e+00\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.2109 - accuracy: 0.0000e+00\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: -3.2417 - accuracy: 0.0000e+00\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.2721 - accuracy: 0.0000e+00\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.3023 - accuracy: 0.0000e+00\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.3329 - accuracy: 0.0000e+00\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.3628 - accuracy: 0.0000e+00\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.3936 - accuracy: 0.0000e+00\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.4240 - accuracy: 0.0000e+00\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.4536 - accuracy: 0.0000e+00\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.4846 - accuracy: 0.0000e+00\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.5147 - accuracy: 0.0000e+00\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.5451 - accuracy: 0.0000e+00\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.5749 - accuracy: 0.0000e+00\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.6060 - accuracy: 0.0000e+00\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.6362 - accuracy: 0.0000e+00\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.6658 - accuracy: 0.0000e+00\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -3.6967 - accuracy: 0.0000e+00\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.7270 - accuracy: 0.0000e+00\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.7579 - accuracy: 0.0000e+00\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.7876 - accuracy: 0.0000e+00\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.8177 - accuracy: 0.0000e+00\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.8484 - accuracy: 0.0000e+00\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.8792 - accuracy: 0.0000e+00\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -3.9092 - accuracy: 0.0000e+00\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -3.9400 - accuracy: 0.0000e+00\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -3.9701 - accuracy: 0.0000e+00\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.0005 - accuracy: 0.0000e+00\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.0309 - accuracy: 0.0000e+00\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.0612 - accuracy: 0.0000e+00\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.0916 - accuracy: 0.0000e+00\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.1218 - accuracy: 0.0000e+00\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.1524 - accuracy: 0.0000e+00\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.1827 - accuracy: 0.0000e+00\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.2132 - accuracy: 0.0000e+00\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.2434 - accuracy: 0.0000e+00\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.2735 - accuracy: 0.0000e+00\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.3038 - accuracy: 0.0000e+00\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.3341 - accuracy: 0.0000e+00\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.3643 - accuracy: 0.0000e+00\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.3948 - accuracy: 0.0000e+00\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: -4.4251 - accuracy: 0.0000e+00\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.4557 - accuracy: 0.0000e+00\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.4861 - accuracy: 0.0000e+00\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.5153 - accuracy: 0.0000e+00\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.5464 - accuracy: 0.0000e+00\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.5769 - accuracy: 0.0000e+00\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.6070 - accuracy: 0.0000e+00\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -4.6378 - accuracy: 0.0000e+00\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -4.6674 - accuracy: 0.0000e+00\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.6981 - accuracy: 0.0000e+00\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.7284 - accuracy: 0.0000e+00\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.7591 - accuracy: 0.0000e+00\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.7890 - accuracy: 0.0000e+00\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.8198 - accuracy: 0.0000e+00\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.8497 - accuracy: 0.0000e+00\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.8802 - accuracy: 0.0000e+00\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.9107 - accuracy: 0.0000e+00\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.9412 - accuracy: 0.0000e+00\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -4.9715 - accuracy: 0.0000e+00\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.0011 - accuracy: 0.0000e+00\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.0319 - accuracy: 0.0000e+00\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.0622 - accuracy: 0.0000e+00\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -5.0923 - accuracy: 0.0000e+00\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -5.1228 - accuracy: 0.0000e+00\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.1532 - accuracy: 0.0000e+00\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.1834 - accuracy: 0.0000e+00\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.2134 - accuracy: 0.0000e+00\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.2441 - accuracy: 0.0000e+00\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.2741 - accuracy: 0.0000e+00\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.3042 - accuracy: 0.0000e+00\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.3343 - accuracy: 0.0000e+00\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -5.3653 - accuracy: 0.0000e+00\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.3955 - accuracy: 0.0000e+00\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.4256 - accuracy: 0.0000e+00\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.4562 - accuracy: 0.0000e+00\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.4858 - accuracy: 0.0000e+00\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.5167 - accuracy: 0.0000e+00\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.5465 - accuracy: 0.0000e+00\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.5770 - accuracy: 0.0000e+00\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.6072 - accuracy: 0.0000e+00\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.6373 - accuracy: 0.0000e+00\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.6681 - accuracy: 0.0000e+00\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -5.6982 - accuracy: 0.0000e+00\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.7279 - accuracy: 0.0000e+00\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.7587 - accuracy: 0.0000e+00\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.7888 - accuracy: 0.0000e+00\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.8191 - accuracy: 0.0000e+00\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.8492 - accuracy: 0.0000e+00\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.8797 - accuracy: 0.0000e+00\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.9098 - accuracy: 0.0000e+00\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.9400 - accuracy: 0.0000e+00\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -5.9700 - accuracy: 0.0000e+00\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.0007 - accuracy: 0.0000e+00\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.0309 - accuracy: 0.0000e+00\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.0613 - accuracy: 0.0000e+00\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.0913 - accuracy: 0.0000e+00\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.1216 - accuracy: 0.0000e+00\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.1518 - accuracy: 0.0000e+00\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.1818 - accuracy: 0.0000e+00\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.2124 - accuracy: 0.0000e+00\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.2427 - accuracy: 0.0000e+00\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.2728 - accuracy: 0.0000e+00\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.3032 - accuracy: 0.0000e+00\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.3331 - accuracy: 0.0000e+00\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.3639 - accuracy: 0.0000e+00\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.3937 - accuracy: 0.0000e+00\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.4240 - accuracy: 0.0000e+00\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.4545 - accuracy: 0.0000e+00\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.4846 - accuracy: 0.0000e+00\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.5150 - accuracy: 0.0000e+00\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.5450 - accuracy: 0.0000e+00\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.5758 - accuracy: 0.0000e+00\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.6058 - accuracy: 0.0000e+00\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.6364 - accuracy: 0.0000e+00\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.6662 - accuracy: 0.0000e+00\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.6968 - accuracy: 0.0000e+00\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.7269 - accuracy: 0.0000e+00\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.7573 - accuracy: 0.0000e+00\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.7877 - accuracy: 0.0000e+00\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.8181 - accuracy: 0.0000e+00\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.8480 - accuracy: 0.0000e+00\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -6.8785 - accuracy: 0.0000e+00\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.9089 - accuracy: 0.0000e+00\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.9390 - accuracy: 0.0000e+00\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -6.9688 - accuracy: 0.0000e+00\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 17ms/step - loss: -6.9996 - accuracy: 0.0000e+00\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.0301 - accuracy: 0.0000e+00\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.0601 - accuracy: 0.0000e+00\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.0897 - accuracy: 0.0000e+00\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.1201 - accuracy: 0.0000e+00\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.1504 - accuracy: 0.0000e+00\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.1805 - accuracy: 0.0000e+00\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.2107 - accuracy: 0.0000e+00\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.2411 - accuracy: 0.0000e+00\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: -7.2706 - accuracy: 0.0000e+00\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.3009 - accuracy: 0.0000e+00\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.3310 - accuracy: 0.0000e+00\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.3609 - accuracy: 0.0000e+00\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: -7.3910 - accuracy: 0.0000e+00\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.4213 - accuracy: 0.0000e+00\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.4509 - accuracy: 0.0000e+00\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.4814 - accuracy: 0.0000e+00\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -7.5117 - accuracy: 0.0000e+00\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.5408 - accuracy: 0.0000e+00\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.5721 - accuracy: 0.0000e+00\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.6016 - accuracy: 0.0000e+00\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.6317 - accuracy: 0.0000e+00\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.6620 - accuracy: 0.0000e+00\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.6922 - accuracy: 0.0000e+00\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.7223 - accuracy: 0.0000e+00\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.7526 - accuracy: 0.0000e+00\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.7830 - accuracy: 0.0000e+00\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.8126 - accuracy: 0.0000e+00\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: -7.8430 - accuracy: 0.0000e+00\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -7.8730 - accuracy: 0.0000e+00\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.9032 - accuracy: 0.0000e+00\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.9333 - accuracy: 0.0000e+00\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.9632 - accuracy: 0.0000e+00\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -7.9933 - accuracy: 0.0000e+00\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.0237 - accuracy: 0.0000e+00\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.0538 - accuracy: 0.0000e+00\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.0834 - accuracy: 0.0000e+00\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.1135 - accuracy: 0.0000e+00\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.1439 - accuracy: 0.0000e+00\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.1736 - accuracy: 0.0000e+00\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.2036 - accuracy: 0.0000e+00\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.2343 - accuracy: 0.0000e+00\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.2640 - accuracy: 0.0000e+00\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.2944 - accuracy: 0.0000e+00\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.3243 - accuracy: 0.0000e+00\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -8.3546 - accuracy: 0.0000e+00\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.3843 - accuracy: 0.0000e+00\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.4147 - accuracy: 0.0000e+00\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: -8.4447 - accuracy: 0.0000e+00\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: -8.4750 - accuracy: 0.0000e+00\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.5051 - accuracy: 0.0000e+00\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.5355 - accuracy: 0.0000e+00\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.5653 - accuracy: 0.0000e+00\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.5955 - accuracy: 0.0000e+00\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -8.6260 - accuracy: 0.0000e+00\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.6559 - accuracy: 0.0000e+00\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.6868 - accuracy: 0.0000e+00\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -8.7164 - accuracy: 0.0000e+00\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.7466 - accuracy: 0.0000e+00\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -8.7773 - accuracy: 0.0000e+00\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -8.8076 - accuracy: 0.0000e+00\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.8377 - accuracy: 0.0000e+00\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -8.8680 - accuracy: 0.0000e+00\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.8981 - accuracy: 0.0000e+00\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -8.9287 - accuracy: 0.0000e+00\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.9586 - accuracy: 0.0000e+00\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -8.9895 - accuracy: 0.0000e+00\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -9.0195 - accuracy: 0.0000e+00\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -9.0499 - accuracy: 0.0000e+00\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -9.0798 - accuracy: 0.0000e+00\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -9.1101 - accuracy: 0.0000e+00\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.1405 - accuracy: 0.0000e+00\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.1709 - accuracy: 0.0000e+00\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.2013 - accuracy: 0.0000e+00\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.2313 - accuracy: 0.0000e+00\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.2614 - accuracy: 0.0000e+00\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.2917 - accuracy: 0.0000e+00\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.3220 - accuracy: 0.0000e+00\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.3519 - accuracy: 0.0000e+00\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.3821 - accuracy: 0.0000e+00\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.4124 - accuracy: 0.0000e+00\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.4428 - accuracy: 0.0000e+00\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.4726 - accuracy: 0.0000e+00\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.5028 - accuracy: 0.0000e+00\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.5327 - accuracy: 0.0000e+00\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.5625 - accuracy: 0.0000e+00\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.5927 - accuracy: 0.0000e+00\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.6229 - accuracy: 0.0000e+00\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.6532 - accuracy: 0.0000e+00\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.6830 - accuracy: 0.0000e+00\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.7127 - accuracy: 0.0000e+00\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.7430 - accuracy: 0.0000e+00\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.7728 - accuracy: 0.0000e+00\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.8033 - accuracy: 0.0000e+00\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -9.8333 - accuracy: 0.0000e+00\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.8628 - accuracy: 0.0000e+00\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.8931 - accuracy: 0.0000e+00\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.9234 - accuracy: 0.0000e+00\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -9.9535 - accuracy: 0.0000e+00\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -9.9838 - accuracy: 0.0000e+00\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -10.0133 - accuracy: 0.0000e+00\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -10.0440 - accuracy: 0.0000e+00\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.0735 - accuracy: 0.0000e+00\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.1042 - accuracy: 0.0000e+00\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.1341 - accuracy: 0.0000e+00\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -10.1641 - accuracy: 0.0000e+00\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.1944 - accuracy: 0.0000e+00\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.2242 - accuracy: 0.0000e+00\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.2549 - accuracy: 0.0000e+00\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -10.2843 - accuracy: 0.0000e+00\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.3146 - accuracy: 0.0000e+00\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.3449 - accuracy: 0.0000e+00\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.3748 - accuracy: 0.0000e+00\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.4046 - accuracy: 0.0000e+00\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.4351 - accuracy: 0.0000e+00\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.4649 - accuracy: 0.0000e+00\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.4947 - accuracy: 0.0000e+00\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.5250 - accuracy: 0.0000e+00\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.5548 - accuracy: 0.0000e+00\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.5845 - accuracy: 0.0000e+00\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.6148 - accuracy: 0.0000e+00\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.6447 - accuracy: 0.0000e+00\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -10.6746 - accuracy: 0.0000e+00\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.7047 - accuracy: 0.0000e+00\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.7343 - accuracy: 0.0000e+00\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.7645 - accuracy: 0.0000e+00\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.7946 - accuracy: 0.0000e+00\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -10.8244 - accuracy: 0.0000e+00\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -10.8545 - accuracy: 0.0000e+00\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.8844 - accuracy: 0.0000e+00\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.9148 - accuracy: 0.0000e+00\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 16ms/step - loss: -10.9446 - accuracy: 0.0000e+00\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -10.9742 - accuracy: 0.0000e+00\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -11.0046 - accuracy: 0.0000e+00\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.0350 - accuracy: 0.0000e+00\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.0646 - accuracy: 0.0000e+00\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.0954 - accuracy: 0.0000e+00\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.1253 - accuracy: 0.0000e+00\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.1549 - accuracy: 0.0000e+00\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.1854 - accuracy: 0.0000e+00\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.2157 - accuracy: 0.0000e+00\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -11.2460 - accuracy: 0.0000e+00\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.2768 - accuracy: 0.0000e+00\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.3065 - accuracy: 0.0000e+00\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.3364 - accuracy: 0.0000e+00\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.3671 - accuracy: 0.0000e+00\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.3972 - accuracy: 0.0000e+00\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.4272 - accuracy: 0.0000e+00\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -11.4575 - accuracy: 0.0000e+00\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.4882 - accuracy: 0.0000e+00\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.5174 - accuracy: 0.0000e+00\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.5480 - accuracy: 0.0000e+00\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.5782 - accuracy: 0.0000e+00\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.6080 - accuracy: 0.0000e+00\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.6377 - accuracy: 0.0000e+00\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.6680 - accuracy: 0.0000e+00\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.6977 - accuracy: 0.0000e+00\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.7280 - accuracy: 0.0000e+00\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.7579 - accuracy: 0.0000e+00\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.7877 - accuracy: 0.0000e+00\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -11.8179 - accuracy: 0.0000e+00\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.8470 - accuracy: 0.0000e+00\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -11.8774 - accuracy: 0.0000e+00\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -11.9072 - accuracy: 0.0000e+00\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.9377 - accuracy: 0.0000e+00\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.9669 - accuracy: 0.0000e+00\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -11.9970 - accuracy: 0.0000e+00\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.0270 - accuracy: 0.0000e+00\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.0570 - accuracy: 0.0000e+00\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.0867 - accuracy: 0.0000e+00\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.1165 - accuracy: 0.0000e+00\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -12.1470 - accuracy: 0.0000e+00\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.1763 - accuracy: 0.0000e+00\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.2064 - accuracy: 0.0000e+00\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.2364 - accuracy: 0.0000e+00\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.2666 - accuracy: 0.0000e+00\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -12.2961 - accuracy: 0.0000e+00\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -12.3263 - accuracy: 0.0000e+00\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.3563 - accuracy: 0.0000e+00\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.3857 - accuracy: 0.0000e+00\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.4161 - accuracy: 0.0000e+00\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -12.4460 - accuracy: 0.0000e+00\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: -12.4758 - accuracy: 0.0000e+00\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -12.5061 - accuracy: 0.0000e+00\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -12.5355 - accuracy: 0.0000e+00\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.5656 - accuracy: 0.0000e+00\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -12.5957 - accuracy: 0.0000e+00\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.6259 - accuracy: 0.0000e+00\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.6553 - accuracy: 0.0000e+00\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.6851 - accuracy: 0.0000e+00\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.7150 - accuracy: 0.0000e+00\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -12.7453 - accuracy: 0.0000e+00\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.7752 - accuracy: 0.0000e+00\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -12.8049 - accuracy: 0.0000e+00\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -12.8348 - accuracy: 0.0000e+00\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.8651 - accuracy: 0.0000e+00\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -12.8949 - accuracy: 0.0000e+00\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -12.9244 - accuracy: 0.0000e+00\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -12.9549 - accuracy: 0.0000e+00\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -12.9849 - accuracy: 0.0000e+00\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: -13.0147 - accuracy: 0.0000e+00\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.0446 - accuracy: 0.0000e+00\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -13.0746 - accuracy: 0.0000e+00\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -13.1050 - accuracy: 0.0000e+00\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -13.1348 - accuracy: 0.0000e+00\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -13.1648 - accuracy: 0.0000e+00\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.1943 - accuracy: 0.0000e+00\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.2246 - accuracy: 0.0000e+00\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.2544 - accuracy: 0.0000e+00\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -13.2847 - accuracy: 0.0000e+00\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -13.3142 - accuracy: 0.0000e+00\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.3441 - accuracy: 0.0000e+00\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.3736 - accuracy: 0.0000e+00\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.4044 - accuracy: 0.0000e+00\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -13.4341 - accuracy: 0.0000e+00\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.4637 - accuracy: 0.0000e+00\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.4933 - accuracy: 0.0000e+00\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -13.5232 - accuracy: 0.0000e+00\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.5534 - accuracy: 0.0000e+00\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -13.5830 - accuracy: 0.0000e+00\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.6129 - accuracy: 0.0000e+00\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.6430 - accuracy: 0.0000e+00\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -13.6726 - accuracy: 0.0000e+00\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -13.7024 - accuracy: 0.0000e+00\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -13.7314 - accuracy: 0.0000e+00\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -13.7620 - accuracy: 0.0000e+00\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.7920 - accuracy: 0.0000e+00\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.8213 - accuracy: 0.0000e+00\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.8511 - accuracy: 0.0000e+00\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.8811 - accuracy: 0.0000e+00\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.9110 - accuracy: 0.0000e+00\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -13.9407 - accuracy: 0.0000e+00\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -13.9705 - accuracy: 0.0000e+00\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -14.0007 - accuracy: 0.0000e+00\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.0299 - accuracy: 0.0000e+00\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.0597 - accuracy: 0.0000e+00\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.0902 - accuracy: 0.0000e+00\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.1201 - accuracy: 0.0000e+00\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.1498 - accuracy: 0.0000e+00\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: -14.1797 - accuracy: 0.0000e+00\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.2097 - accuracy: 0.0000e+00\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.2399 - accuracy: 0.0000e+00\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -14.2697 - accuracy: 0.0000e+00\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.2998 - accuracy: 0.0000e+00\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.3298 - accuracy: 0.0000e+00\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: -14.3599 - accuracy: 0.0000e+00\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: -14.3898 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x163c81730>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality **After `fit()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 12:34:19.108279: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>initial_pred</th>\n",
       "      <th>pred_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.73024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.73024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.73024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.73024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.73024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  initial_pred  pred_after_fit\n",
       "abbrev                                     \n",
       "AL       18.8           0.5         0.73024\n",
       "AK       18.1           0.5         0.73024\n",
       "AZ       18.6           0.5         0.73024\n",
       "AR       22.4           0.5         0.73024\n",
       "CA       12.0           0.5         0.73024"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.6043488 ],\n",
       "        [-0.06645387],\n",
       "        [ 0.49008608],\n",
       "        [ 0.5926229 ],\n",
       "        [ 0.6011095 ],\n",
       "        [-0.48289979],\n",
       "        [ 0.21052301],\n",
       "        [-0.03000039],\n",
       "        [ 0.23048395],\n",
       "        [-0.6410422 ],\n",
       "        [ 0.66076636],\n",
       "        [ 0.37151527]], dtype=float32),\n",
       " array([0.99584275], dtype=float32)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - They are synonyms:\n",
    "> - Cost | Error | Loss\n",
    "> - https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sparse_categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mean_absolute_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mean_squared_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=206\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=206\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the `loss` accordingly to your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=6, activation='relu', kernel_initializer='zeros'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 250.4451 - mse: 250.4451\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.4297 - mse: 250.4297\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.4145 - mse: 250.4145\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.3993 - mse: 250.3993\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.3840 - mse: 250.3840\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.3684 - mse: 250.3684\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.3534 - mse: 250.3534\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.3380 - mse: 250.3380\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.3227 - mse: 250.3227\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.3075 - mse: 250.3075\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.2920 - mse: 250.2920\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.2769 - mse: 250.2769\n",
      "Epoch 13/500\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 250.9527 - mse: 250.9527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 12:37:14.818974: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step - loss: 250.2616 - mse: 250.2616\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.2464 - mse: 250.2464\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.2309 - mse: 250.2309\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.2157 - mse: 250.2157\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.2004 - mse: 250.2004\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.1852 - mse: 250.1852\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.1699 - mse: 250.1699\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.1547 - mse: 250.1547\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.1393 - mse: 250.1393\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.1241 - mse: 250.1241\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.1089 - mse: 250.1089\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.0935 - mse: 250.0935\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.0784 - mse: 250.0784\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 250.0630 - mse: 250.0630\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.0477 - mse: 250.0477\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.0328 - mse: 250.0328\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.0173 - mse: 250.0173\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 250.0019 - mse: 250.0019\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.9866 - mse: 249.9866\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.9715 - mse: 249.9715\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.9563 - mse: 249.9563\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.9409 - mse: 249.9409\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.9254 - mse: 249.9254\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.9107 - mse: 249.9107\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.8950 - mse: 249.8950\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.8799 - mse: 249.8799\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.8647 - mse: 249.8647\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.8493 - mse: 249.8493\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.8342 - mse: 249.8342\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.8189 - mse: 249.8189\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.8038 - mse: 249.8038\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.7886 - mse: 249.7886\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.7736 - mse: 249.7736\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.7580 - mse: 249.7580\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.7429 - mse: 249.7429\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.7280 - mse: 249.7280\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.7125 - mse: 249.7125\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.6975 - mse: 249.6975\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.6823 - mse: 249.6823\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.6671 - mse: 249.6671\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.6519 - mse: 249.6519\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.6368 - mse: 249.6368\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.6214 - mse: 249.6214\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.6066 - mse: 249.6066\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.5911 - mse: 249.5911\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 249.5758 - mse: 249.5758\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.5609 - mse: 249.5609\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.5458 - mse: 249.5458\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.5305 - mse: 249.5305\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.5154 - mse: 249.5154\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.5003 - mse: 249.5003\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.4850 - mse: 249.4850\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.4698 - mse: 249.4698\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.4550 - mse: 249.4550\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.4397 - mse: 249.4397\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.4243 - mse: 249.4243\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.4094 - mse: 249.4094\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.3944 - mse: 249.3944\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.3789 - mse: 249.3789\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.3642 - mse: 249.3642\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.3489 - mse: 249.3489\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.3336 - mse: 249.3336\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.3183 - mse: 249.3183\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.3034 - mse: 249.3034\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.2879 - mse: 249.2879\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.2730 - mse: 249.2730\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.2577 - mse: 249.2577\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.2428 - mse: 249.2428\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.2273 - mse: 249.2273\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.2124 - mse: 249.2124\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 249.1973 - mse: 249.1973\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.1821 - mse: 249.1821\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.1669 - mse: 249.1669\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.1517 - mse: 249.1517\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.1367 - mse: 249.1367\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.1220 - mse: 249.1220\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.1069 - mse: 249.1069\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.0916 - mse: 249.0916\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 249.0766 - mse: 249.0766\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 249.0620 - mse: 249.0620\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.0466 - mse: 249.0466\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.0318 - mse: 249.0318\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.0170 - mse: 249.0170\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 249.0019 - mse: 249.0019\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.9868 - mse: 248.9868\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.9722 - mse: 248.9722\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.9568 - mse: 248.9568\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.9420 - mse: 248.9420\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.9273 - mse: 248.9273\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.9122 - mse: 248.9122\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.8972 - mse: 248.8972\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.8822 - mse: 248.8822\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.8674 - mse: 248.8674\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 248.8526 - mse: 248.8526\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.8375 - mse: 248.8375\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.8227 - mse: 248.8227\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.8076 - mse: 248.8076\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.7932 - mse: 248.7932\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 248.7780 - mse: 248.7780\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.7631 - mse: 248.7631\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.7482 - mse: 248.7482\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 248.7336 - mse: 248.7336\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.7187 - mse: 248.7187\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.7039 - mse: 248.7039\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.6889 - mse: 248.6889\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.6741 - mse: 248.6741\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.6594 - mse: 248.6593\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.6446 - mse: 248.6446\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.6299 - mse: 248.6299\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.6149 - mse: 248.6149\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.6003 - mse: 248.6003\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.5853 - mse: 248.5853\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.5705 - mse: 248.5705\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.5556 - mse: 248.5556\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.5411 - mse: 248.5411\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.5262 - mse: 248.5262\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.5113 - mse: 248.5113\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.4967 - mse: 248.4967\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.4818 - mse: 248.4818\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.4670 - mse: 248.4670\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.4521 - mse: 248.4521\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.4374 - mse: 248.4374\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.4226 - mse: 248.4226\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.4080 - mse: 248.4080\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.3933 - mse: 248.3933\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.3785 - mse: 248.3785\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.3634 - mse: 248.3634\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.3490 - mse: 248.3490\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.3343 - mse: 248.3343\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.3196 - mse: 248.3196\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 248.3049 - mse: 248.3049\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.2902 - mse: 248.2902\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.2755 - mse: 248.2755\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.2607 - mse: 248.2607\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.2462 - mse: 248.2462\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.2316 - mse: 248.2316\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.2170 - mse: 248.2170\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.2022 - mse: 248.2022\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.1876 - mse: 248.1876\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.1731 - mse: 248.1731\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.1582 - mse: 248.1582\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.1439 - mse: 248.1439\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.1290 - mse: 248.1290\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.1145 - mse: 248.1145\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.0999 - mse: 248.0999\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.0852 - mse: 248.0852\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.0707 - mse: 248.0707\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.0562 - mse: 248.0562\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.0413 - mse: 248.0413\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 248.0268 - mse: 248.0268\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 248.0121 - mse: 248.0121\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.9976 - mse: 247.9976\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.9830 - mse: 247.9830\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.9685 - mse: 247.9685\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.9538 - mse: 247.9538\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.9393 - mse: 247.9393\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.9248 - mse: 247.9248\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.9102 - mse: 247.9102\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.8958 - mse: 247.8958\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.8812 - mse: 247.8812\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.8667 - mse: 247.8667\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.8523 - mse: 247.8523\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.8377 - mse: 247.8377\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.8235 - mse: 247.8235\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.8085 - mse: 247.8085\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.7944 - mse: 247.7944\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.7800 - mse: 247.7800\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.7656 - mse: 247.7656\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.7511 - mse: 247.7511\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.7367 - mse: 247.7367\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.7222 - mse: 247.7222\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.7078 - mse: 247.7078\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.6937 - mse: 247.6937\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.6790 - mse: 247.6790\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.6649 - mse: 247.6649\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.6503 - mse: 247.6503\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.6361 - mse: 247.6361\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.6216 - mse: 247.6216\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.6074 - mse: 247.6074\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.5932 - mse: 247.5932\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.5787 - mse: 247.5787\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.5644 - mse: 247.5644\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.5502 - mse: 247.5502\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.5358 - mse: 247.5358\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.5213 - mse: 247.5213\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.5073 - mse: 247.5073\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.4929 - mse: 247.4929\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.4786 - mse: 247.4786\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.4646 - mse: 247.4646\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.4500 - mse: 247.4500\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.4357 - mse: 247.4357\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.4216 - mse: 247.4216\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.4073 - mse: 247.4073\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.3931 - mse: 247.3931\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.3788 - mse: 247.3788\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.3645 - mse: 247.3645\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.3503 - mse: 247.3503\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.3363 - mse: 247.3363\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.3220 - mse: 247.3220\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.3079 - mse: 247.3079\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.2938 - mse: 247.2938\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.2797 - mse: 247.2797\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.2655 - mse: 247.2655\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.2517 - mse: 247.2517\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.2376 - mse: 247.2376\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.2235 - mse: 247.2235\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.2095 - mse: 247.2095\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.1957 - mse: 247.1957\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.1817 - mse: 247.1817\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.1676 - mse: 247.1676\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.1538 - mse: 247.1538\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.1398 - mse: 247.1398\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.1261 - mse: 247.1261\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.1122 - mse: 247.1122\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.0982 - mse: 247.0982\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.0845 - mse: 247.0845\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.0706 - mse: 247.0706\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 247.0568 - mse: 247.0568\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.0430 - mse: 247.0430\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.0291 - mse: 247.0291\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 247.0151 - mse: 247.0151\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 247.0012 - mse: 247.0012\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.9874 - mse: 246.9874\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.9738 - mse: 246.9738\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.9598 - mse: 246.9598\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.9459 - mse: 246.9459\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.9322 - mse: 246.9322\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.9182 - mse: 246.9182\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.9043 - mse: 246.9043\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.8906 - mse: 246.8906\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.8768 - mse: 246.8768\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.8629 - mse: 246.8629\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.8492 - mse: 246.8492\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.8354 - mse: 246.8354\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.8218 - mse: 246.8218\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.8078 - mse: 246.8078\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.7940 - mse: 246.7940\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.7804 - mse: 246.7804\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.7669 - mse: 246.7669\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.7528 - mse: 246.7528\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.7392 - mse: 246.7392\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 246.7255 - mse: 246.7255\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.7118 - mse: 246.7118\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.6982 - mse: 246.6982\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.6843 - mse: 246.6843\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.6709 - mse: 246.6709\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.6573 - mse: 246.6573\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.6433 - mse: 246.6433\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.6296 - mse: 246.6296\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.6163 - mse: 246.6163\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.6026 - mse: 246.6026\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.5891 - mse: 246.5891\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.5754 - mse: 246.5754\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.5617 - mse: 246.5617\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.5483 - mse: 246.5483\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.5346 - mse: 246.5346\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.5210 - mse: 246.5210\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.5074 - mse: 246.5074\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 246.4941 - mse: 246.4941\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.4805 - mse: 246.4805\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.4670 - mse: 246.4670\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.4533 - mse: 246.4533\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.4400 - mse: 246.4400\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.4265 - mse: 246.4265\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.4130 - mse: 246.4130\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.3994 - mse: 246.3994\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.3860 - mse: 246.3860\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.3727 - mse: 246.3727\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.3590 - mse: 246.3590\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.3459 - mse: 246.3459\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.3324 - mse: 246.3324\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.3192 - mse: 246.3192\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.3056 - mse: 246.3056\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.2923 - mse: 246.2923\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.2791 - mse: 246.2791\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.2657 - mse: 246.2657\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.2525 - mse: 246.2525\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.2391 - mse: 246.2391\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.2258 - mse: 246.2258\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.2125 - mse: 246.2125\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.1992 - mse: 246.1992\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.1861 - mse: 246.1861\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.1728 - mse: 246.1728\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.1595 - mse: 246.1595\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.1463 - mse: 246.1463\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.1328 - mse: 246.1328\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.1199 - mse: 246.1199\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.1064 - mse: 246.1064\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.0934 - mse: 246.0934\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.0802 - mse: 246.0802\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.0669 - mse: 246.0669\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.0540 - mse: 246.0540\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.0407 - mse: 246.0407\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.0274 - mse: 246.0274\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 246.0144 - mse: 246.0144\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 246.0013 - mse: 246.0013\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.9884 - mse: 245.9884\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.9752 - mse: 245.9752\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.9622 - mse: 245.9622\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.9491 - mse: 245.9491\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.9363 - mse: 245.9363\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.9233 - mse: 245.9233\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.9100 - mse: 245.9100\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.8971 - mse: 245.8971\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.8844 - mse: 245.8844\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.8713 - mse: 245.8713\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.8584 - mse: 245.8584\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.8456 - mse: 245.8456\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.8323 - mse: 245.8323\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.8198 - mse: 245.8198\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.8067 - mse: 245.8067\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.7940 - mse: 245.7940\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.7810 - mse: 245.7810\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.7680 - mse: 245.7680\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.7553 - mse: 245.7553\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.7424 - mse: 245.7424\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.7298 - mse: 245.7298\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.7170 - mse: 245.7170\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.7040 - mse: 245.7040\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.6915 - mse: 245.6915\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.6785 - mse: 245.6785\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.6659 - mse: 245.6659\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.6530 - mse: 245.6530\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.6406 - mse: 245.6406\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.6277 - mse: 245.6277\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.6151 - mse: 245.6151\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.6023 - mse: 245.6023\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.5897 - mse: 245.5897\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.5770 - mse: 245.5770\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.5642 - mse: 245.5642\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.5518 - mse: 245.5518\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.5392 - mse: 245.5392\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.5263 - mse: 245.5263\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.5136 - mse: 245.5136\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.5012 - mse: 245.5012\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.4886 - mse: 245.4886\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.4761 - mse: 245.4761\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.4635 - mse: 245.4635\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.4509 - mse: 245.4509\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.4382 - mse: 245.4382\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.4258 - mse: 245.4258\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.4133 - mse: 245.4133\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.4007 - mse: 245.4007\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.3881 - mse: 245.3881\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.3758 - mse: 245.3758\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.3631 - mse: 245.3631\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.3506 - mse: 245.3506\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.3382 - mse: 245.3382\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.3258 - mse: 245.3258\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.3134 - mse: 245.3134\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.3008 - mse: 245.3008\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.2885 - mse: 245.2885\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.2762 - mse: 245.2762\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.2637 - mse: 245.2637\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.2512 - mse: 245.2512\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.2391 - mse: 245.2391\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.2266 - mse: 245.2266\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.2143 - mse: 245.2143\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.2020 - mse: 245.2020\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.1897 - mse: 245.1897\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.1774 - mse: 245.1774\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.1651 - mse: 245.1651\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.1527 - mse: 245.1527\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.1404 - mse: 245.1404\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.1284 - mse: 245.1284\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.1160 - mse: 245.1160\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.1040 - mse: 245.1039\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.0916 - mse: 245.0916\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.0795 - mse: 245.0795\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.0671 - mse: 245.0671\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.0552 - mse: 245.0552\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.0431 - mse: 245.0431\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.0309 - mse: 245.0309\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 245.0189 - mse: 245.0189\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 245.0067 - mse: 245.0067\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.9947 - mse: 244.9947\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.9827 - mse: 244.9827\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 244.9706 - mse: 244.9706\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.9584 - mse: 244.9584\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.9465 - mse: 244.9465\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.9347 - mse: 244.9347\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.9224 - mse: 244.9224\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 244.9106 - mse: 244.9106\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.8984 - mse: 244.8984\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.8866 - mse: 244.8866\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 244.8745 - mse: 244.8745\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.8627 - mse: 244.8627\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.8507 - mse: 244.8507\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.8386 - mse: 244.8386\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.8270 - mse: 244.8270\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.8149 - mse: 244.8149\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.8031 - mse: 244.8031\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.7912 - mse: 244.7912\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.7792 - mse: 244.7792\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.7674 - mse: 244.7674\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.7556 - mse: 244.7556\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.7438 - mse: 244.7438\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.7320 - mse: 244.7320\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.7200 - mse: 244.7200\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 244.7084 - mse: 244.7084\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.6964 - mse: 244.6964\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.6847 - mse: 244.6847\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.6732 - mse: 244.6732\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.6612 - mse: 244.6612\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.6496 - mse: 244.6496\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.6379 - mse: 244.6379\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.6263 - mse: 244.6263\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.6145 - mse: 244.6145\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.6028 - mse: 244.6028\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.5912 - mse: 244.5912\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.5797 - mse: 244.5797\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.5680 - mse: 244.5680\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.5564 - mse: 244.5564\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.5447 - mse: 244.5447\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 244.5332 - mse: 244.5332\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 244.5217 - mse: 244.5217\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 244.5101 - mse: 244.5101\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.4984 - mse: 244.4984\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.4869 - mse: 244.4869\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.4753 - mse: 244.4753\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.4638 - mse: 244.4638\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.4524 - mse: 244.4524\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.4409 - mse: 244.4409\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.4293 - mse: 244.4293\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.4178 - mse: 244.4178\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.4064 - mse: 244.4064\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.3951 - mse: 244.3951\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.3835 - mse: 244.3835\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.3722 - mse: 244.3722\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.3609 - mse: 244.3609\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.3495 - mse: 244.3495\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.3382 - mse: 244.3382\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.3267 - mse: 244.3267\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.3153 - mse: 244.3153\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.3041 - mse: 244.3041\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.2928 - mse: 244.2928\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 244.2817 - mse: 244.2817\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.2704 - mse: 244.2704\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.2592 - mse: 244.2592\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.2477 - mse: 244.2477\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.2365 - mse: 244.2365\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.2254 - mse: 244.2254\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.2141 - mse: 244.2141\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.2031 - mse: 244.2031\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.1916 - mse: 244.1916\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.1805 - mse: 244.1805\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.1692 - mse: 244.1692\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.1582 - mse: 244.1582\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.1470 - mse: 244.1470\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.1359 - mse: 244.1359\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.1245 - mse: 244.1245\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.1135 - mse: 244.1135\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.1023 - mse: 244.1023\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.0914 - mse: 244.0914\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.0802 - mse: 244.0802\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.0692 - mse: 244.0692\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 244.0580 - mse: 244.0580\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 244.0469 - mse: 244.0469\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.0362 - mse: 244.0362\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.0250 - mse: 244.0250\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.0140 - mse: 244.0140\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 244.0029 - mse: 244.0029\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.9919 - mse: 243.9919\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.9810 - mse: 243.9810\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.9699 - mse: 243.9699\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.9592 - mse: 243.9592\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 216.1059 - mse: 216.105 - 0s 5ms/step - loss: 243.9479 - mse: 243.9479\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 243.9371 - mse: 243.9371\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.9261 - mse: 243.9261\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 243.9153 - mse: 243.9153\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.9043 - mse: 243.9043\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.8932 - mse: 243.8932\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.8828 - mse: 243.8828\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.8717 - mse: 243.8717\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 243.8608 - mse: 243.8608\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.8501 - mse: 243.8501\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 243.8393 - mse: 243.8393\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 243.8286 - mse: 243.8286\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.8177 - mse: 243.8177\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.8070 - mse: 243.8070\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.7963 - mse: 243.7963\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.7857 - mse: 243.7857\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 243.7751 - mse: 243.7751\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.7643 - mse: 243.7643\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 243.7537 - mse: 243.7537\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.7431 - mse: 243.7431\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.7324 - mse: 243.7324\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 243.7219 - mse: 243.7219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1645e16a0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - [ ] Why is the model not improving as it iterates (**deep**ly **learn**s)?\n",
    "> - [ ] How can we solve this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=29\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=29\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=182\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=182\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure `linear` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=6, activation='relu', kernel_initializer='zeros'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the `weights` (numbers) on the Mathematical Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.0037806 ],\n",
       "        [-0.09828407],\n",
       "        [ 0.3429786 ],\n",
       "        [-0.4515835 ],\n",
       "        [-0.23184407],\n",
       "        [ 0.10979211],\n",
       "        [-0.38510633],\n",
       "        [-0.6206576 ],\n",
       "        [-0.5009832 ],\n",
       "        [-0.48230162],\n",
       "        [ 0.5879469 ],\n",
       "        [ 0.2460075 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 12:55:48.293657: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel = df[['total']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_initial</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_initial\n",
       "abbrev                     \n",
       "AL       18.8           0.0\n",
       "AK       18.1           0.0\n",
       "AZ       18.6           0.0\n",
       "AR       22.4           0.0\n",
       "CA       12.0           0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_initial'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compile()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 - 0s - loss: 265.9763 - mse: 265.9763\n",
      "Epoch 2/500\n",
      "2/2 - 0s - loss: 265.9126 - mse: 265.9126\n",
      "Epoch 3/500\n",
      "2/2 - 0s - loss: 265.8492 - mse: 265.8492\n",
      "Epoch 4/500\n",
      "2/2 - 0s - loss: 265.7868 - mse: 265.7868\n",
      "Epoch 5/500\n",
      "2/2 - 0s - loss: 265.7239 - mse: 265.7239\n",
      "Epoch 6/500\n",
      "2/2 - 0s - loss: 265.6612 - mse: 265.6612\n",
      "Epoch 7/500\n",
      "2/2 - 0s - loss: 265.5970 - mse: 265.5970\n",
      "Epoch 8/500\n",
      "2/2 - 0s - loss: 265.5343 - mse: 265.5343\n",
      "Epoch 9/500\n",
      "2/2 - 0s - loss: 265.4720 - mse: 265.4720\n",
      "Epoch 10/500\n",
      "2/2 - 0s - loss: 265.4084 - mse: 265.4084\n",
      "Epoch 11/500\n",
      "2/2 - 0s - loss: 265.3456 - mse: 265.3456\n",
      "Epoch 12/500\n",
      "2/2 - 0s - loss: 265.2827 - mse: 265.2827\n",
      "Epoch 13/500\n",
      "2/2 - 0s - loss: 265.2204 - mse: 265.2204\n",
      "Epoch 14/500\n",
      "2/2 - 0s - loss: 265.1571 - mse: 265.1571\n",
      "Epoch 15/500\n",
      "2/2 - 0s - loss: 265.0932 - mse: 265.0932\n",
      "Epoch 16/500\n",
      "2/2 - 0s - loss: 265.0304 - mse: 265.0304\n",
      "Epoch 17/500\n",
      "2/2 - 0s - loss: 264.9682 - mse: 264.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 12:56:58.603572: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "2/2 - 0s - loss: 264.9058 - mse: 264.9058\n",
      "Epoch 19/500\n",
      "2/2 - 0s - loss: 264.8427 - mse: 264.8427\n",
      "Epoch 20/500\n",
      "2/2 - 0s - loss: 264.7797 - mse: 264.7797\n",
      "Epoch 21/500\n",
      "2/2 - 0s - loss: 264.7170 - mse: 264.7170\n",
      "Epoch 22/500\n",
      "2/2 - 0s - loss: 264.6543 - mse: 264.6543\n",
      "Epoch 23/500\n",
      "2/2 - 0s - loss: 264.5913 - mse: 264.5913\n",
      "Epoch 24/500\n",
      "2/2 - 0s - loss: 264.5283 - mse: 264.5283\n",
      "Epoch 25/500\n",
      "2/2 - 0s - loss: 264.4660 - mse: 264.4660\n",
      "Epoch 26/500\n",
      "2/2 - 0s - loss: 264.4038 - mse: 264.4038\n",
      "Epoch 27/500\n",
      "2/2 - 0s - loss: 264.3411 - mse: 264.3411\n",
      "Epoch 28/500\n",
      "2/2 - 0s - loss: 264.2783 - mse: 264.2783\n",
      "Epoch 29/500\n",
      "2/2 - 0s - loss: 264.2149 - mse: 264.2149\n",
      "Epoch 30/500\n",
      "2/2 - 0s - loss: 264.1523 - mse: 264.1523\n",
      "Epoch 31/500\n",
      "2/2 - 0s - loss: 264.0900 - mse: 264.0900\n",
      "Epoch 32/500\n",
      "2/2 - 0s - loss: 264.0270 - mse: 264.0270\n",
      "Epoch 33/500\n",
      "2/2 - 0s - loss: 263.9641 - mse: 263.9641\n",
      "Epoch 34/500\n",
      "2/2 - 0s - loss: 263.9008 - mse: 263.9008\n",
      "Epoch 35/500\n",
      "2/2 - 0s - loss: 263.8385 - mse: 263.8386\n",
      "Epoch 36/500\n",
      "2/2 - 0s - loss: 263.7762 - mse: 263.7762\n",
      "Epoch 37/500\n",
      "2/2 - 0s - loss: 263.7127 - mse: 263.7127\n",
      "Epoch 38/500\n",
      "2/2 - 0s - loss: 263.6509 - mse: 263.6509\n",
      "Epoch 39/500\n",
      "2/2 - 0s - loss: 263.5866 - mse: 263.5866\n",
      "Epoch 40/500\n",
      "2/2 - 0s - loss: 263.5244 - mse: 263.5244\n",
      "Epoch 41/500\n",
      "2/2 - 0s - loss: 263.4613 - mse: 263.4613\n",
      "Epoch 42/500\n",
      "2/2 - 0s - loss: 263.3998 - mse: 263.3998\n",
      "Epoch 43/500\n",
      "2/2 - 0s - loss: 263.3358 - mse: 263.3358\n",
      "Epoch 44/500\n",
      "2/2 - 0s - loss: 263.2740 - mse: 263.2740\n",
      "Epoch 45/500\n",
      "2/2 - 0s - loss: 263.2110 - mse: 263.2110\n",
      "Epoch 46/500\n",
      "2/2 - 0s - loss: 263.1482 - mse: 263.1482\n",
      "Epoch 47/500\n",
      "2/2 - 0s - loss: 263.0859 - mse: 263.0859\n",
      "Epoch 48/500\n",
      "2/2 - 0s - loss: 263.0228 - mse: 263.0228\n",
      "Epoch 49/500\n",
      "2/2 - 0s - loss: 262.9606 - mse: 262.9606\n",
      "Epoch 50/500\n",
      "2/2 - 0s - loss: 262.8982 - mse: 262.8982\n",
      "Epoch 51/500\n",
      "2/2 - 0s - loss: 262.8349 - mse: 262.8349\n",
      "Epoch 52/500\n",
      "2/2 - 0s - loss: 262.7726 - mse: 262.7726\n",
      "Epoch 53/500\n",
      "2/2 - 0s - loss: 262.7096 - mse: 262.7096\n",
      "Epoch 54/500\n",
      "2/2 - 0s - loss: 262.6471 - mse: 262.6471\n",
      "Epoch 55/500\n",
      "2/2 - 0s - loss: 262.5846 - mse: 262.5846\n",
      "Epoch 56/500\n",
      "2/2 - 0s - loss: 262.5217 - mse: 262.5216\n",
      "Epoch 57/500\n",
      "2/2 - 0s - loss: 262.4591 - mse: 262.4591\n",
      "Epoch 58/500\n",
      "2/2 - 0s - loss: 262.3958 - mse: 262.3958\n",
      "Epoch 59/500\n",
      "2/2 - 0s - loss: 262.3344 - mse: 262.3344\n",
      "Epoch 60/500\n",
      "2/2 - 0s - loss: 262.2702 - mse: 262.2702\n",
      "Epoch 61/500\n",
      "2/2 - 0s - loss: 262.2087 - mse: 262.2087\n",
      "Epoch 62/500\n",
      "2/2 - 0s - loss: 262.1452 - mse: 262.1452\n",
      "Epoch 63/500\n",
      "2/2 - 0s - loss: 262.0843 - mse: 262.0843\n",
      "Epoch 64/500\n",
      "2/2 - 0s - loss: 262.0201 - mse: 262.0201\n",
      "Epoch 65/500\n",
      "2/2 - 0s - loss: 261.9569 - mse: 261.9569\n",
      "Epoch 66/500\n",
      "2/2 - 0s - loss: 261.8955 - mse: 261.8955\n",
      "Epoch 67/500\n",
      "2/2 - 0s - loss: 261.8323 - mse: 261.8323\n",
      "Epoch 68/500\n",
      "2/2 - 0s - loss: 261.7702 - mse: 261.7702\n",
      "Epoch 69/500\n",
      "2/2 - 0s - loss: 261.7080 - mse: 261.7080\n",
      "Epoch 70/500\n",
      "2/2 - 0s - loss: 261.6454 - mse: 261.6454\n",
      "Epoch 71/500\n",
      "2/2 - 0s - loss: 261.5825 - mse: 261.5825\n",
      "Epoch 72/500\n",
      "2/2 - 0s - loss: 261.5205 - mse: 261.5205\n",
      "Epoch 73/500\n",
      "2/2 - 0s - loss: 261.4581 - mse: 261.4581\n",
      "Epoch 74/500\n",
      "2/2 - 0s - loss: 261.3960 - mse: 261.3960\n",
      "Epoch 75/500\n",
      "2/2 - 0s - loss: 261.3332 - mse: 261.3332\n",
      "Epoch 76/500\n",
      "2/2 - 0s - loss: 261.2714 - mse: 261.2714\n",
      "Epoch 77/500\n",
      "2/2 - 0s - loss: 261.2096 - mse: 261.2096\n",
      "Epoch 78/500\n",
      "2/2 - 0s - loss: 261.1470 - mse: 261.1469\n",
      "Epoch 79/500\n",
      "2/2 - 0s - loss: 261.0845 - mse: 261.0845\n",
      "Epoch 80/500\n",
      "2/2 - 0s - loss: 261.0224 - mse: 261.0224\n",
      "Epoch 81/500\n",
      "2/2 - 0s - loss: 260.9606 - mse: 260.9606\n",
      "Epoch 82/500\n",
      "2/2 - 0s - loss: 260.8984 - mse: 260.8984\n",
      "Epoch 83/500\n",
      "2/2 - 0s - loss: 260.8358 - mse: 260.8358\n",
      "Epoch 84/500\n",
      "2/2 - 0s - loss: 260.7747 - mse: 260.7747\n",
      "Epoch 85/500\n",
      "2/2 - 0s - loss: 260.7116 - mse: 260.7116\n",
      "Epoch 86/500\n",
      "2/2 - 0s - loss: 260.6490 - mse: 260.6490\n",
      "Epoch 87/500\n",
      "2/2 - 0s - loss: 260.5889 - mse: 260.5889\n",
      "Epoch 88/500\n",
      "2/2 - 0s - loss: 260.5255 - mse: 260.5255\n",
      "Epoch 89/500\n",
      "2/2 - 0s - loss: 260.4630 - mse: 260.4630\n",
      "Epoch 90/500\n",
      "2/2 - 0s - loss: 260.4018 - mse: 260.4018\n",
      "Epoch 91/500\n",
      "2/2 - 0s - loss: 260.3391 - mse: 260.3391\n",
      "Epoch 92/500\n",
      "2/2 - 0s - loss: 260.2780 - mse: 260.2780\n",
      "Epoch 93/500\n",
      "2/2 - 0s - loss: 260.2155 - mse: 260.2155\n",
      "Epoch 94/500\n",
      "2/2 - 0s - loss: 260.1538 - mse: 260.1538\n",
      "Epoch 95/500\n",
      "2/2 - 0s - loss: 260.0920 - mse: 260.0920\n",
      "Epoch 96/500\n",
      "2/2 - 0s - loss: 260.0296 - mse: 260.0296\n",
      "Epoch 97/500\n",
      "2/2 - 0s - loss: 259.9672 - mse: 259.9672\n",
      "Epoch 98/500\n",
      "2/2 - 0s - loss: 259.9054 - mse: 259.9054\n",
      "Epoch 99/500\n",
      "2/2 - 0s - loss: 259.8436 - mse: 259.8436\n",
      "Epoch 100/500\n",
      "2/2 - 0s - loss: 259.7824 - mse: 259.7824\n",
      "Epoch 101/500\n",
      "2/2 - 0s - loss: 259.7195 - mse: 259.7195\n",
      "Epoch 102/500\n",
      "2/2 - 0s - loss: 259.6576 - mse: 259.6576\n",
      "Epoch 103/500\n",
      "2/2 - 0s - loss: 259.5957 - mse: 259.5957\n",
      "Epoch 104/500\n",
      "2/2 - 0s - loss: 259.5342 - mse: 259.5342\n",
      "Epoch 105/500\n",
      "2/2 - 0s - loss: 259.4729 - mse: 259.4729\n",
      "Epoch 106/500\n",
      "2/2 - 0s - loss: 259.4102 - mse: 259.4102\n",
      "Epoch 107/500\n",
      "2/2 - 0s - loss: 259.3490 - mse: 259.3490\n",
      "Epoch 108/500\n",
      "2/2 - 0s - loss: 259.2868 - mse: 259.2868\n",
      "Epoch 109/500\n",
      "2/2 - 0s - loss: 259.2260 - mse: 259.2260\n",
      "Epoch 110/500\n",
      "2/2 - 0s - loss: 259.1638 - mse: 259.1638\n",
      "Epoch 111/500\n",
      "2/2 - 0s - loss: 259.1023 - mse: 259.1023\n",
      "Epoch 112/500\n",
      "2/2 - 0s - loss: 259.0406 - mse: 259.0406\n",
      "Epoch 113/500\n",
      "2/2 - 0s - loss: 258.9797 - mse: 258.9797\n",
      "Epoch 114/500\n",
      "2/2 - 0s - loss: 258.9184 - mse: 258.9184\n",
      "Epoch 115/500\n",
      "2/2 - 0s - loss: 258.8568 - mse: 258.8568\n",
      "Epoch 116/500\n",
      "2/2 - 0s - loss: 258.7952 - mse: 258.7952\n",
      "Epoch 117/500\n",
      "2/2 - 0s - loss: 258.7337 - mse: 258.7337\n",
      "Epoch 118/500\n",
      "2/2 - 0s - loss: 258.6717 - mse: 258.6717\n",
      "Epoch 119/500\n",
      "2/2 - 0s - loss: 258.6103 - mse: 258.6103\n",
      "Epoch 120/500\n",
      "2/2 - 0s - loss: 258.5496 - mse: 258.5496\n",
      "Epoch 121/500\n",
      "2/2 - 0s - loss: 258.4871 - mse: 258.4871\n",
      "Epoch 122/500\n",
      "2/2 - 0s - loss: 258.4265 - mse: 258.4265\n",
      "Epoch 123/500\n",
      "2/2 - 0s - loss: 258.3642 - mse: 258.3642\n",
      "Epoch 124/500\n",
      "2/2 - 0s - loss: 258.3023 - mse: 258.3023\n",
      "Epoch 125/500\n",
      "2/2 - 0s - loss: 258.2411 - mse: 258.2411\n",
      "Epoch 126/500\n",
      "2/2 - 0s - loss: 258.1801 - mse: 258.1801\n",
      "Epoch 127/500\n",
      "2/2 - 0s - loss: 258.1176 - mse: 258.1176\n",
      "Epoch 128/500\n",
      "2/2 - 0s - loss: 258.0551 - mse: 258.0551\n",
      "Epoch 129/500\n",
      "2/2 - 0s - loss: 257.9947 - mse: 257.9947\n",
      "Epoch 130/500\n",
      "2/2 - 0s - loss: 257.9326 - mse: 257.9326\n",
      "Epoch 131/500\n",
      "2/2 - 0s - loss: 257.8709 - mse: 257.8709\n",
      "Epoch 132/500\n",
      "2/2 - 0s - loss: 257.8088 - mse: 257.8088\n",
      "Epoch 133/500\n",
      "2/2 - 0s - loss: 257.7478 - mse: 257.7478\n",
      "Epoch 134/500\n",
      "2/2 - 0s - loss: 257.6859 - mse: 257.6859\n",
      "Epoch 135/500\n",
      "2/2 - 0s - loss: 257.6247 - mse: 257.6247\n",
      "Epoch 136/500\n",
      "2/2 - 0s - loss: 257.5629 - mse: 257.5629\n",
      "Epoch 137/500\n",
      "2/2 - 0s - loss: 257.5020 - mse: 257.5020\n",
      "Epoch 138/500\n",
      "2/2 - 0s - loss: 257.4400 - mse: 257.4400\n",
      "Epoch 139/500\n",
      "2/2 - 0s - loss: 257.3787 - mse: 257.3787\n",
      "Epoch 140/500\n",
      "2/2 - 0s - loss: 257.3164 - mse: 257.3164\n",
      "Epoch 141/500\n",
      "2/2 - 0s - loss: 257.2561 - mse: 257.2561\n",
      "Epoch 142/500\n",
      "2/2 - 0s - loss: 257.1941 - mse: 257.1941\n",
      "Epoch 143/500\n",
      "2/2 - 0s - loss: 257.1331 - mse: 257.1331\n",
      "Epoch 144/500\n",
      "2/2 - 0s - loss: 257.0714 - mse: 257.0714\n",
      "Epoch 145/500\n",
      "2/2 - 0s - loss: 257.0103 - mse: 257.0103\n",
      "Epoch 146/500\n",
      "2/2 - 0s - loss: 256.9488 - mse: 256.9488\n",
      "Epoch 147/500\n",
      "2/2 - 0s - loss: 256.8864 - mse: 256.8864\n",
      "Epoch 148/500\n",
      "2/2 - 0s - loss: 256.8257 - mse: 256.8257\n",
      "Epoch 149/500\n",
      "2/2 - 0s - loss: 256.7638 - mse: 256.7638\n",
      "Epoch 150/500\n",
      "2/2 - 0s - loss: 256.7018 - mse: 256.7018\n",
      "Epoch 151/500\n",
      "2/2 - 0s - loss: 256.6407 - mse: 256.6407\n",
      "Epoch 152/500\n",
      "2/2 - 0s - loss: 256.5799 - mse: 256.5799\n",
      "Epoch 153/500\n",
      "2/2 - 0s - loss: 256.5180 - mse: 256.5180\n",
      "Epoch 154/500\n",
      "2/2 - 0s - loss: 256.4553 - mse: 256.4553\n",
      "Epoch 155/500\n",
      "2/2 - 0s - loss: 256.3950 - mse: 256.3950\n",
      "Epoch 156/500\n",
      "2/2 - 0s - loss: 256.3332 - mse: 256.3332\n",
      "Epoch 157/500\n",
      "2/2 - 0s - loss: 256.2713 - mse: 256.2713\n",
      "Epoch 158/500\n",
      "2/2 - 0s - loss: 256.2100 - mse: 256.2100\n",
      "Epoch 159/500\n",
      "2/2 - 0s - loss: 256.1487 - mse: 256.1487\n",
      "Epoch 160/500\n",
      "2/2 - 0s - loss: 256.0879 - mse: 256.0879\n",
      "Epoch 161/500\n",
      "2/2 - 0s - loss: 256.0252 - mse: 256.0252\n",
      "Epoch 162/500\n",
      "2/2 - 0s - loss: 255.9652 - mse: 255.9652\n",
      "Epoch 163/500\n",
      "2/2 - 0s - loss: 255.9026 - mse: 255.9026\n",
      "Epoch 164/500\n",
      "2/2 - 0s - loss: 255.8417 - mse: 255.8417\n",
      "Epoch 165/500\n",
      "2/2 - 0s - loss: 255.7807 - mse: 255.7807\n",
      "Epoch 166/500\n",
      "2/2 - 0s - loss: 255.7183 - mse: 255.7183\n",
      "Epoch 167/500\n",
      "2/2 - 0s - loss: 255.6572 - mse: 255.6572\n",
      "Epoch 168/500\n",
      "2/2 - 0s - loss: 255.5970 - mse: 255.5970\n",
      "Epoch 169/500\n",
      "2/2 - 0s - loss: 255.5337 - mse: 255.5336\n",
      "Epoch 170/500\n",
      "2/2 - 0s - loss: 255.4736 - mse: 255.4736\n",
      "Epoch 171/500\n",
      "2/2 - 0s - loss: 255.4115 - mse: 255.4115\n",
      "Epoch 172/500\n",
      "2/2 - 0s - loss: 255.3510 - mse: 255.3510\n",
      "Epoch 173/500\n",
      "2/2 - 0s - loss: 255.2886 - mse: 255.2886\n",
      "Epoch 174/500\n",
      "2/2 - 0s - loss: 255.2275 - mse: 255.2275\n",
      "Epoch 175/500\n",
      "2/2 - 0s - loss: 255.1669 - mse: 255.1669\n",
      "Epoch 176/500\n",
      "2/2 - 0s - loss: 255.1048 - mse: 255.1048\n",
      "Epoch 177/500\n",
      "2/2 - 0s - loss: 255.0440 - mse: 255.0440\n",
      "Epoch 178/500\n",
      "2/2 - 0s - loss: 254.9825 - mse: 254.9825\n",
      "Epoch 179/500\n",
      "2/2 - 0s - loss: 254.9213 - mse: 254.9213\n",
      "Epoch 180/500\n",
      "2/2 - 0s - loss: 254.8607 - mse: 254.8607\n",
      "Epoch 181/500\n",
      "2/2 - 0s - loss: 254.7992 - mse: 254.7992\n",
      "Epoch 182/500\n",
      "2/2 - 0s - loss: 254.7388 - mse: 254.7388\n",
      "Epoch 183/500\n",
      "2/2 - 0s - loss: 254.6769 - mse: 254.6769\n",
      "Epoch 184/500\n",
      "2/2 - 0s - loss: 254.6166 - mse: 254.6166\n",
      "Epoch 185/500\n",
      "2/2 - 0s - loss: 254.5542 - mse: 254.5542\n",
      "Epoch 186/500\n",
      "2/2 - 0s - loss: 254.4948 - mse: 254.4948\n",
      "Epoch 187/500\n",
      "2/2 - 0s - loss: 254.4336 - mse: 254.4336\n",
      "Epoch 188/500\n",
      "2/2 - 0s - loss: 254.3719 - mse: 254.3719\n",
      "Epoch 189/500\n",
      "2/2 - 0s - loss: 254.3119 - mse: 254.3119\n",
      "Epoch 190/500\n",
      "2/2 - 0s - loss: 254.2502 - mse: 254.2502\n",
      "Epoch 191/500\n",
      "2/2 - 0s - loss: 254.1895 - mse: 254.1895\n",
      "Epoch 192/500\n",
      "2/2 - 0s - loss: 254.1291 - mse: 254.1291\n",
      "Epoch 193/500\n",
      "2/2 - 0s - loss: 254.0669 - mse: 254.0669\n",
      "Epoch 194/500\n",
      "2/2 - 0s - loss: 254.0063 - mse: 254.0063\n",
      "Epoch 195/500\n",
      "2/2 - 0s - loss: 253.9457 - mse: 253.9457\n",
      "Epoch 196/500\n",
      "2/2 - 0s - loss: 253.8846 - mse: 253.8846\n",
      "Epoch 197/500\n",
      "2/2 - 0s - loss: 253.8241 - mse: 253.8241\n",
      "Epoch 198/500\n",
      "2/2 - 0s - loss: 253.7624 - mse: 253.7624\n",
      "Epoch 199/500\n",
      "2/2 - 0s - loss: 253.7014 - mse: 253.7014\n",
      "Epoch 200/500\n",
      "2/2 - 0s - loss: 253.6413 - mse: 253.6413\n",
      "Epoch 201/500\n",
      "2/2 - 0s - loss: 253.5807 - mse: 253.5807\n",
      "Epoch 202/500\n",
      "2/2 - 0s - loss: 253.5195 - mse: 253.5195\n",
      "Epoch 203/500\n",
      "2/2 - 0s - loss: 253.4578 - mse: 253.4578\n",
      "Epoch 204/500\n",
      "2/2 - 0s - loss: 253.3964 - mse: 253.3964\n",
      "Epoch 205/500\n",
      "2/2 - 0s - loss: 253.3375 - mse: 253.3375\n",
      "Epoch 206/500\n",
      "2/2 - 0s - loss: 253.2760 - mse: 253.2760\n",
      "Epoch 207/500\n",
      "2/2 - 0s - loss: 253.2153 - mse: 253.2153\n",
      "Epoch 208/500\n",
      "2/2 - 0s - loss: 253.1539 - mse: 253.1539\n",
      "Epoch 209/500\n",
      "2/2 - 0s - loss: 253.0938 - mse: 253.0938\n",
      "Epoch 210/500\n",
      "2/2 - 0s - loss: 253.0336 - mse: 253.0336\n",
      "Epoch 211/500\n",
      "2/2 - 0s - loss: 252.9721 - mse: 252.9721\n",
      "Epoch 212/500\n",
      "2/2 - 0s - loss: 252.9116 - mse: 252.9116\n",
      "Epoch 213/500\n",
      "2/2 - 0s - loss: 252.8508 - mse: 252.8508\n",
      "Epoch 214/500\n",
      "2/2 - 0s - loss: 252.7904 - mse: 252.7904\n",
      "Epoch 215/500\n",
      "2/2 - 0s - loss: 252.7296 - mse: 252.7296\n",
      "Epoch 216/500\n",
      "2/2 - 0s - loss: 252.6701 - mse: 252.6701\n",
      "Epoch 217/500\n",
      "2/2 - 0s - loss: 252.6085 - mse: 252.6085\n",
      "Epoch 218/500\n",
      "2/2 - 0s - loss: 252.5480 - mse: 252.5480\n",
      "Epoch 219/500\n",
      "2/2 - 0s - loss: 252.4874 - mse: 252.4873\n",
      "Epoch 220/500\n",
      "2/2 - 0s - loss: 252.4274 - mse: 252.4274\n",
      "Epoch 221/500\n",
      "2/2 - 0s - loss: 252.3664 - mse: 252.3664\n",
      "Epoch 222/500\n",
      "2/2 - 0s - loss: 252.3060 - mse: 252.3060\n",
      "Epoch 223/500\n",
      "2/2 - 0s - loss: 252.2460 - mse: 252.2460\n",
      "Epoch 224/500\n",
      "2/2 - 0s - loss: 252.1853 - mse: 252.1853\n",
      "Epoch 225/500\n",
      "2/2 - 0s - loss: 252.1249 - mse: 252.1249\n",
      "Epoch 226/500\n",
      "2/2 - 0s - loss: 252.0641 - mse: 252.0641\n",
      "Epoch 227/500\n",
      "2/2 - 0s - loss: 252.0035 - mse: 252.0035\n",
      "Epoch 228/500\n",
      "2/2 - 0s - loss: 251.9441 - mse: 251.9441\n",
      "Epoch 229/500\n",
      "2/2 - 0s - loss: 251.8836 - mse: 251.8836\n",
      "Epoch 230/500\n",
      "2/2 - 0s - loss: 251.8230 - mse: 251.8230\n",
      "Epoch 231/500\n",
      "2/2 - 0s - loss: 251.7622 - mse: 251.7622\n",
      "Epoch 232/500\n",
      "2/2 - 0s - loss: 251.7030 - mse: 251.7030\n",
      "Epoch 233/500\n",
      "2/2 - 0s - loss: 251.6421 - mse: 251.6421\n",
      "Epoch 234/500\n",
      "2/2 - 0s - loss: 251.5814 - mse: 251.5814\n",
      "Epoch 235/500\n",
      "2/2 - 0s - loss: 251.5223 - mse: 251.5223\n",
      "Epoch 236/500\n",
      "2/2 - 0s - loss: 251.4614 - mse: 251.4614\n",
      "Epoch 237/500\n",
      "2/2 - 0s - loss: 251.4008 - mse: 251.4008\n",
      "Epoch 238/500\n",
      "2/2 - 0s - loss: 251.3406 - mse: 251.3406\n",
      "Epoch 239/500\n",
      "2/2 - 0s - loss: 251.2805 - mse: 251.2805\n",
      "Epoch 240/500\n",
      "2/2 - 0s - loss: 251.2205 - mse: 251.2205\n",
      "Epoch 241/500\n",
      "2/2 - 0s - loss: 251.1601 - mse: 251.1601\n",
      "Epoch 242/500\n",
      "2/2 - 0s - loss: 251.1004 - mse: 251.1004\n",
      "Epoch 243/500\n",
      "2/2 - 0s - loss: 251.0408 - mse: 251.0408\n",
      "Epoch 244/500\n",
      "2/2 - 0s - loss: 250.9802 - mse: 250.9802\n",
      "Epoch 245/500\n",
      "2/2 - 0s - loss: 250.9194 - mse: 250.9194\n",
      "Epoch 246/500\n",
      "2/2 - 0s - loss: 250.8601 - mse: 250.8601\n",
      "Epoch 247/500\n",
      "2/2 - 0s - loss: 250.7998 - mse: 250.7998\n",
      "Epoch 248/500\n",
      "2/2 - 0s - loss: 250.7397 - mse: 250.7397\n",
      "Epoch 249/500\n",
      "2/2 - 0s - loss: 250.6798 - mse: 250.6798\n",
      "Epoch 250/500\n",
      "2/2 - 0s - loss: 250.6196 - mse: 250.6196\n",
      "Epoch 251/500\n",
      "2/2 - 0s - loss: 250.5599 - mse: 250.5599\n",
      "Epoch 252/500\n",
      "2/2 - 0s - loss: 250.4991 - mse: 250.4991\n",
      "Epoch 253/500\n",
      "2/2 - 0s - loss: 250.4395 - mse: 250.4395\n",
      "Epoch 254/500\n",
      "2/2 - 0s - loss: 250.3796 - mse: 250.3796\n",
      "Epoch 255/500\n",
      "2/2 - 0s - loss: 250.3195 - mse: 250.3195\n",
      "Epoch 256/500\n",
      "2/2 - 0s - loss: 250.2581 - mse: 250.2581\n",
      "Epoch 257/500\n",
      "2/2 - 0s - loss: 250.2000 - mse: 250.2000\n",
      "Epoch 258/500\n",
      "2/2 - 0s - loss: 250.1389 - mse: 250.1389\n",
      "Epoch 259/500\n",
      "2/2 - 0s - loss: 250.0786 - mse: 250.0786\n",
      "Epoch 260/500\n",
      "2/2 - 0s - loss: 250.0197 - mse: 250.0197\n",
      "Epoch 261/500\n",
      "2/2 - 0s - loss: 249.9595 - mse: 249.9595\n",
      "Epoch 262/500\n",
      "2/2 - 0s - loss: 249.8997 - mse: 249.8997\n",
      "Epoch 263/500\n",
      "2/2 - 0s - loss: 249.8388 - mse: 249.8388\n",
      "Epoch 264/500\n",
      "2/2 - 0s - loss: 249.7783 - mse: 249.7783\n",
      "Epoch 265/500\n",
      "2/2 - 0s - loss: 249.7207 - mse: 249.7207\n",
      "Epoch 266/500\n",
      "2/2 - 0s - loss: 249.6597 - mse: 249.6597\n",
      "Epoch 267/500\n",
      "2/2 - 0s - loss: 249.5999 - mse: 249.5999\n",
      "Epoch 268/500\n",
      "2/2 - 0s - loss: 249.5395 - mse: 249.5395\n",
      "Epoch 269/500\n",
      "2/2 - 0s - loss: 249.4794 - mse: 249.4794\n",
      "Epoch 270/500\n",
      "2/2 - 0s - loss: 249.4204 - mse: 249.4204\n",
      "Epoch 271/500\n",
      "2/2 - 0s - loss: 249.3605 - mse: 249.3605\n",
      "Epoch 272/500\n",
      "2/2 - 0s - loss: 249.2991 - mse: 249.2991\n",
      "Epoch 273/500\n",
      "2/2 - 0s - loss: 249.2393 - mse: 249.2393\n",
      "Epoch 274/500\n",
      "2/2 - 0s - loss: 249.1801 - mse: 249.1801\n",
      "Epoch 275/500\n",
      "2/2 - 0s - loss: 249.1205 - mse: 249.1205\n",
      "Epoch 276/500\n",
      "2/2 - 0s - loss: 249.0591 - mse: 249.0591\n",
      "Epoch 277/500\n",
      "2/2 - 0s - loss: 248.9993 - mse: 248.9993\n",
      "Epoch 278/500\n",
      "2/2 - 0s - loss: 248.9395 - mse: 248.9395\n",
      "Epoch 279/500\n",
      "2/2 - 0s - loss: 248.8802 - mse: 248.8802\n",
      "Epoch 280/500\n",
      "2/2 - 0s - loss: 248.8194 - mse: 248.8194\n",
      "Epoch 281/500\n",
      "2/2 - 0s - loss: 248.7591 - mse: 248.7591\n",
      "Epoch 282/500\n",
      "2/2 - 0s - loss: 248.7001 - mse: 248.7001\n",
      "Epoch 283/500\n",
      "2/2 - 0s - loss: 248.6386 - mse: 248.6386\n",
      "Epoch 284/500\n",
      "2/2 - 0s - loss: 248.5790 - mse: 248.5790\n",
      "Epoch 285/500\n",
      "2/2 - 0s - loss: 248.5205 - mse: 248.5205\n",
      "Epoch 286/500\n",
      "2/2 - 0s - loss: 248.4595 - mse: 248.4595\n",
      "Epoch 287/500\n",
      "2/2 - 0s - loss: 248.3992 - mse: 248.3992\n",
      "Epoch 288/500\n",
      "2/2 - 0s - loss: 248.3402 - mse: 248.3402\n",
      "Epoch 289/500\n",
      "2/2 - 0s - loss: 248.2808 - mse: 248.2808\n",
      "Epoch 290/500\n",
      "2/2 - 0s - loss: 248.2194 - mse: 248.2194\n",
      "Epoch 291/500\n",
      "2/2 - 0s - loss: 248.1608 - mse: 248.1608\n",
      "Epoch 292/500\n",
      "2/2 - 0s - loss: 248.1003 - mse: 248.1003\n",
      "Epoch 293/500\n",
      "2/2 - 0s - loss: 248.0404 - mse: 248.0404\n",
      "Epoch 294/500\n",
      "2/2 - 0s - loss: 247.9809 - mse: 247.9809\n",
      "Epoch 295/500\n",
      "2/2 - 0s - loss: 247.9213 - mse: 247.9213\n",
      "Epoch 296/500\n",
      "2/2 - 0s - loss: 247.8615 - mse: 247.8615\n",
      "Epoch 297/500\n",
      "2/2 - 0s - loss: 247.8009 - mse: 247.8009\n",
      "Epoch 298/500\n",
      "2/2 - 0s - loss: 247.7412 - mse: 247.7412\n",
      "Epoch 299/500\n",
      "2/2 - 0s - loss: 247.6815 - mse: 247.6815\n",
      "Epoch 300/500\n",
      "2/2 - 0s - loss: 247.6223 - mse: 247.6223\n",
      "Epoch 301/500\n",
      "2/2 - 0s - loss: 247.5618 - mse: 247.5618\n",
      "Epoch 302/500\n",
      "2/2 - 0s - loss: 247.5020 - mse: 247.5020\n",
      "Epoch 303/500\n",
      "2/2 - 0s - loss: 247.4425 - mse: 247.4425\n",
      "Epoch 304/500\n",
      "2/2 - 0s - loss: 247.3824 - mse: 247.3824\n",
      "Epoch 305/500\n",
      "2/2 - 0s - loss: 247.3225 - mse: 247.3225\n",
      "Epoch 306/500\n",
      "2/2 - 0s - loss: 247.2634 - mse: 247.2634\n",
      "Epoch 307/500\n",
      "2/2 - 0s - loss: 247.2047 - mse: 247.2047\n",
      "Epoch 308/500\n",
      "2/2 - 0s - loss: 247.1442 - mse: 247.1442\n",
      "Epoch 309/500\n",
      "2/2 - 0s - loss: 247.0840 - mse: 247.0840\n",
      "Epoch 310/500\n",
      "2/2 - 0s - loss: 247.0252 - mse: 247.0252\n",
      "Epoch 311/500\n",
      "2/2 - 0s - loss: 246.9658 - mse: 246.9658\n",
      "Epoch 312/500\n",
      "2/2 - 0s - loss: 246.9055 - mse: 246.9055\n",
      "Epoch 313/500\n",
      "2/2 - 0s - loss: 246.8466 - mse: 246.8466\n",
      "Epoch 314/500\n",
      "2/2 - 0s - loss: 246.7865 - mse: 246.7865\n",
      "Epoch 315/500\n",
      "2/2 - 0s - loss: 246.7273 - mse: 246.7273\n",
      "Epoch 316/500\n",
      "2/2 - 0s - loss: 246.6678 - mse: 246.6678\n",
      "Epoch 317/500\n",
      "2/2 - 0s - loss: 246.6084 - mse: 246.6084\n",
      "Epoch 318/500\n",
      "2/2 - 0s - loss: 246.5476 - mse: 246.5476\n",
      "Epoch 319/500\n",
      "2/2 - 0s - loss: 246.4887 - mse: 246.4887\n",
      "Epoch 320/500\n",
      "2/2 - 0s - loss: 246.4286 - mse: 246.4286\n",
      "Epoch 321/500\n",
      "2/2 - 0s - loss: 246.3692 - mse: 246.3692\n",
      "Epoch 322/500\n",
      "2/2 - 0s - loss: 246.3109 - mse: 246.3109\n",
      "Epoch 323/500\n",
      "2/2 - 0s - loss: 246.2499 - mse: 246.2499\n",
      "Epoch 324/500\n",
      "2/2 - 0s - loss: 246.1904 - mse: 246.1904\n",
      "Epoch 325/500\n",
      "2/2 - 0s - loss: 246.1312 - mse: 246.1312\n",
      "Epoch 326/500\n",
      "2/2 - 0s - loss: 246.0720 - mse: 246.0720\n",
      "Epoch 327/500\n",
      "2/2 - 0s - loss: 246.0122 - mse: 246.0122\n",
      "Epoch 328/500\n",
      "2/2 - 0s - loss: 245.9518 - mse: 245.9518\n",
      "Epoch 329/500\n",
      "2/2 - 0s - loss: 245.8925 - mse: 245.8925\n",
      "Epoch 330/500\n",
      "2/2 - 0s - loss: 245.8342 - mse: 245.8342\n",
      "Epoch 331/500\n",
      "2/2 - 0s - loss: 245.7736 - mse: 245.7736\n",
      "Epoch 332/500\n",
      "2/2 - 0s - loss: 245.7141 - mse: 245.7141\n",
      "Epoch 333/500\n",
      "2/2 - 0s - loss: 245.6543 - mse: 245.6543\n",
      "Epoch 334/500\n",
      "2/2 - 0s - loss: 245.5948 - mse: 245.5948\n",
      "Epoch 335/500\n",
      "2/2 - 0s - loss: 245.5352 - mse: 245.5352\n",
      "Epoch 336/500\n",
      "2/2 - 0s - loss: 245.4758 - mse: 245.4758\n",
      "Epoch 337/500\n",
      "2/2 - 0s - loss: 245.4158 - mse: 245.4158\n",
      "Epoch 338/500\n",
      "2/2 - 0s - loss: 245.3566 - mse: 245.3566\n",
      "Epoch 339/500\n",
      "2/2 - 0s - loss: 245.2967 - mse: 245.2967\n",
      "Epoch 340/500\n",
      "2/2 - 0s - loss: 245.2373 - mse: 245.2373\n",
      "Epoch 341/500\n",
      "2/2 - 0s - loss: 245.1781 - mse: 245.1781\n",
      "Epoch 342/500\n",
      "2/2 - 0s - loss: 245.1189 - mse: 245.1189\n",
      "Epoch 343/500\n",
      "2/2 - 0s - loss: 245.0590 - mse: 245.0590\n",
      "Epoch 344/500\n",
      "2/2 - 0s - loss: 245.0003 - mse: 245.0003\n",
      "Epoch 345/500\n",
      "2/2 - 0s - loss: 244.9407 - mse: 244.9407\n",
      "Epoch 346/500\n",
      "2/2 - 0s - loss: 244.8806 - mse: 244.8806\n",
      "Epoch 347/500\n",
      "2/2 - 0s - loss: 244.8231 - mse: 244.8231\n",
      "Epoch 348/500\n",
      "2/2 - 0s - loss: 244.7636 - mse: 244.7636\n",
      "Epoch 349/500\n",
      "2/2 - 0s - loss: 244.7036 - mse: 244.7036\n",
      "Epoch 350/500\n",
      "2/2 - 0s - loss: 244.6443 - mse: 244.6443\n",
      "Epoch 351/500\n",
      "2/2 - 0s - loss: 244.5854 - mse: 244.5854\n",
      "Epoch 352/500\n",
      "2/2 - 0s - loss: 244.5259 - mse: 244.5259\n",
      "Epoch 353/500\n",
      "2/2 - 0s - loss: 244.4673 - mse: 244.4673\n",
      "Epoch 354/500\n",
      "2/2 - 0s - loss: 244.4067 - mse: 244.4067\n",
      "Epoch 355/500\n",
      "2/2 - 0s - loss: 244.3491 - mse: 244.3491\n",
      "Epoch 356/500\n",
      "2/2 - 0s - loss: 244.2892 - mse: 244.2892\n",
      "Epoch 357/500\n",
      "2/2 - 0s - loss: 244.2291 - mse: 244.2291\n",
      "Epoch 358/500\n",
      "2/2 - 0s - loss: 244.1697 - mse: 244.1697\n",
      "Epoch 359/500\n",
      "2/2 - 0s - loss: 244.1111 - mse: 244.1111\n",
      "Epoch 360/500\n",
      "2/2 - 0s - loss: 244.0521 - mse: 244.0521\n",
      "Epoch 361/500\n",
      "2/2 - 0s - loss: 243.9922 - mse: 243.9922\n",
      "Epoch 362/500\n",
      "2/2 - 0s - loss: 243.9338 - mse: 243.9338\n",
      "Epoch 363/500\n",
      "2/2 - 0s - loss: 243.8747 - mse: 243.8747\n",
      "Epoch 364/500\n",
      "2/2 - 0s - loss: 243.8148 - mse: 243.8148\n",
      "Epoch 365/500\n",
      "2/2 - 0s - loss: 243.7559 - mse: 243.7559\n",
      "Epoch 366/500\n",
      "2/2 - 0s - loss: 243.6965 - mse: 243.6965\n",
      "Epoch 367/500\n",
      "2/2 - 0s - loss: 243.6375 - mse: 243.6375\n",
      "Epoch 368/500\n",
      "2/2 - 0s - loss: 243.5788 - mse: 243.5788\n",
      "Epoch 369/500\n",
      "2/2 - 0s - loss: 243.5195 - mse: 243.5195\n",
      "Epoch 370/500\n",
      "2/2 - 0s - loss: 243.4605 - mse: 243.4605\n",
      "Epoch 371/500\n",
      "2/2 - 0s - loss: 243.4023 - mse: 243.4023\n",
      "Epoch 372/500\n",
      "2/2 - 0s - loss: 243.3420 - mse: 243.3420\n",
      "Epoch 373/500\n",
      "2/2 - 0s - loss: 243.2829 - mse: 243.2829\n",
      "Epoch 374/500\n",
      "2/2 - 0s - loss: 243.2241 - mse: 243.2241\n",
      "Epoch 375/500\n",
      "2/2 - 0s - loss: 243.1652 - mse: 243.1652\n",
      "Epoch 376/500\n",
      "2/2 - 0s - loss: 243.1051 - mse: 243.1051\n",
      "Epoch 377/500\n",
      "2/2 - 0s - loss: 243.0466 - mse: 243.0466\n",
      "Epoch 378/500\n",
      "2/2 - 0s - loss: 242.9883 - mse: 242.9883\n",
      "Epoch 379/500\n",
      "2/2 - 0s - loss: 242.9278 - mse: 242.9278\n",
      "Epoch 380/500\n",
      "2/2 - 0s - loss: 242.8688 - mse: 242.8688\n",
      "Epoch 381/500\n",
      "2/2 - 0s - loss: 242.8110 - mse: 242.8110\n",
      "Epoch 382/500\n",
      "2/2 - 0s - loss: 242.7523 - mse: 242.7523\n",
      "Epoch 383/500\n",
      "2/2 - 0s - loss: 242.6921 - mse: 242.6921\n",
      "Epoch 384/500\n",
      "2/2 - 0s - loss: 242.6335 - mse: 242.6335\n",
      "Epoch 385/500\n",
      "2/2 - 0s - loss: 242.5750 - mse: 242.5750\n",
      "Epoch 386/500\n",
      "2/2 - 0s - loss: 242.5156 - mse: 242.5156\n",
      "Epoch 387/500\n",
      "2/2 - 0s - loss: 242.4566 - mse: 242.4566\n",
      "Epoch 388/500\n",
      "2/2 - 0s - loss: 242.3982 - mse: 242.3982\n",
      "Epoch 389/500\n",
      "2/2 - 0s - loss: 242.3406 - mse: 242.3406\n",
      "Epoch 390/500\n",
      "2/2 - 0s - loss: 242.2805 - mse: 242.2805\n",
      "Epoch 391/500\n",
      "2/2 - 0s - loss: 242.2215 - mse: 242.2215\n",
      "Epoch 392/500\n",
      "2/2 - 0s - loss: 242.1630 - mse: 242.1630\n",
      "Epoch 393/500\n",
      "2/2 - 0s - loss: 242.1039 - mse: 242.1039\n",
      "Epoch 394/500\n",
      "2/2 - 0s - loss: 242.0456 - mse: 242.0456\n",
      "Epoch 395/500\n",
      "2/2 - 0s - loss: 241.9857 - mse: 241.9857\n",
      "Epoch 396/500\n",
      "2/2 - 0s - loss: 241.9270 - mse: 241.9270\n",
      "Epoch 397/500\n",
      "2/2 - 0s - loss: 241.8681 - mse: 241.8681\n",
      "Epoch 398/500\n",
      "2/2 - 0s - loss: 241.8087 - mse: 241.8087\n",
      "Epoch 399/500\n",
      "2/2 - 0s - loss: 241.7504 - mse: 241.7504\n",
      "Epoch 400/500\n",
      "2/2 - 0s - loss: 241.6916 - mse: 241.6916\n",
      "Epoch 401/500\n",
      "2/2 - 0s - loss: 241.6326 - mse: 241.6326\n",
      "Epoch 402/500\n",
      "2/2 - 0s - loss: 241.5737 - mse: 241.5737\n",
      "Epoch 403/500\n",
      "2/2 - 0s - loss: 241.5156 - mse: 241.5156\n",
      "Epoch 404/500\n",
      "2/2 - 0s - loss: 241.4562 - mse: 241.4562\n",
      "Epoch 405/500\n",
      "2/2 - 0s - loss: 241.3971 - mse: 241.3971\n",
      "Epoch 406/500\n",
      "2/2 - 0s - loss: 241.3387 - mse: 241.3387\n",
      "Epoch 407/500\n",
      "2/2 - 0s - loss: 241.2805 - mse: 241.2805\n",
      "Epoch 408/500\n",
      "2/2 - 0s - loss: 241.2220 - mse: 241.2220\n",
      "Epoch 409/500\n",
      "2/2 - 0s - loss: 241.1635 - mse: 241.1635\n",
      "Epoch 410/500\n",
      "2/2 - 0s - loss: 241.1053 - mse: 241.1053\n",
      "Epoch 411/500\n",
      "2/2 - 0s - loss: 241.0455 - mse: 241.0455\n",
      "Epoch 412/500\n",
      "2/2 - 0s - loss: 240.9880 - mse: 240.9880\n",
      "Epoch 413/500\n",
      "2/2 - 0s - loss: 240.9288 - mse: 240.9288\n",
      "Epoch 414/500\n",
      "2/2 - 0s - loss: 240.8710 - mse: 240.8710\n",
      "Epoch 415/500\n",
      "2/2 - 0s - loss: 240.8119 - mse: 240.8119\n",
      "Epoch 416/500\n",
      "2/2 - 0s - loss: 240.7548 - mse: 240.7548\n",
      "Epoch 417/500\n",
      "2/2 - 0s - loss: 240.6949 - mse: 240.6949\n",
      "Epoch 418/500\n",
      "2/2 - 0s - loss: 240.6367 - mse: 240.6367\n",
      "Epoch 419/500\n",
      "2/2 - 0s - loss: 240.5788 - mse: 240.5788\n",
      "Epoch 420/500\n",
      "2/2 - 0s - loss: 240.5204 - mse: 240.5204\n",
      "Epoch 421/500\n",
      "2/2 - 0s - loss: 240.4621 - mse: 240.4621\n",
      "Epoch 422/500\n",
      "2/2 - 0s - loss: 240.4029 - mse: 240.4029\n",
      "Epoch 423/500\n",
      "2/2 - 0s - loss: 240.3457 - mse: 240.3457\n",
      "Epoch 424/500\n",
      "2/2 - 0s - loss: 240.2864 - mse: 240.2864\n",
      "Epoch 425/500\n",
      "2/2 - 0s - loss: 240.2293 - mse: 240.2293\n",
      "Epoch 426/500\n",
      "2/2 - 0s - loss: 240.1706 - mse: 240.1706\n",
      "Epoch 427/500\n",
      "2/2 - 0s - loss: 240.1115 - mse: 240.1115\n",
      "Epoch 428/500\n",
      "2/2 - 0s - loss: 240.0538 - mse: 240.0538\n",
      "Epoch 429/500\n",
      "2/2 - 0s - loss: 239.9955 - mse: 239.9955\n",
      "Epoch 430/500\n",
      "2/2 - 0s - loss: 239.9367 - mse: 239.9367\n",
      "Epoch 431/500\n",
      "2/2 - 0s - loss: 239.8788 - mse: 239.8788\n",
      "Epoch 432/500\n",
      "2/2 - 0s - loss: 239.8213 - mse: 239.8213\n",
      "Epoch 433/500\n",
      "2/2 - 0s - loss: 239.7609 - mse: 239.7609\n",
      "Epoch 434/500\n",
      "2/2 - 0s - loss: 239.7032 - mse: 239.7032\n",
      "Epoch 435/500\n",
      "2/2 - 0s - loss: 239.6451 - mse: 239.6451\n",
      "Epoch 436/500\n",
      "2/2 - 0s - loss: 239.5869 - mse: 239.5869\n",
      "Epoch 437/500\n",
      "2/2 - 0s - loss: 239.5281 - mse: 239.5281\n",
      "Epoch 438/500\n",
      "2/2 - 0s - loss: 239.4703 - mse: 239.4703\n",
      "Epoch 439/500\n",
      "2/2 - 0s - loss: 239.4114 - mse: 239.4114\n",
      "Epoch 440/500\n",
      "2/2 - 0s - loss: 239.3529 - mse: 239.3529\n",
      "Epoch 441/500\n",
      "2/2 - 0s - loss: 239.2953 - mse: 239.2953\n",
      "Epoch 442/500\n",
      "2/2 - 0s - loss: 239.2358 - mse: 239.2358\n",
      "Epoch 443/500\n",
      "2/2 - 0s - loss: 239.1787 - mse: 239.1787\n",
      "Epoch 444/500\n",
      "2/2 - 0s - loss: 239.1206 - mse: 239.1206\n",
      "Epoch 445/500\n",
      "2/2 - 0s - loss: 239.0612 - mse: 239.0612\n",
      "Epoch 446/500\n",
      "2/2 - 0s - loss: 239.0025 - mse: 239.0025\n",
      "Epoch 447/500\n",
      "2/2 - 0s - loss: 238.9457 - mse: 238.9457\n",
      "Epoch 448/500\n",
      "2/2 - 0s - loss: 238.8863 - mse: 238.8863\n",
      "Epoch 449/500\n",
      "2/2 - 0s - loss: 238.8284 - mse: 238.8284\n",
      "Epoch 450/500\n",
      "2/2 - 0s - loss: 238.7707 - mse: 238.7707\n",
      "Epoch 451/500\n",
      "2/2 - 0s - loss: 238.7116 - mse: 238.7116\n",
      "Epoch 452/500\n",
      "2/2 - 0s - loss: 238.6540 - mse: 238.6540\n",
      "Epoch 453/500\n",
      "2/2 - 0s - loss: 238.5945 - mse: 238.5945\n",
      "Epoch 454/500\n",
      "2/2 - 0s - loss: 238.5368 - mse: 238.5368\n",
      "Epoch 455/500\n",
      "2/2 - 0s - loss: 238.4795 - mse: 238.4795\n",
      "Epoch 456/500\n",
      "2/2 - 0s - loss: 238.4212 - mse: 238.4212\n",
      "Epoch 457/500\n",
      "2/2 - 0s - loss: 238.3627 - mse: 238.3627\n",
      "Epoch 458/500\n",
      "2/2 - 0s - loss: 238.3044 - mse: 238.3044\n",
      "Epoch 459/500\n",
      "2/2 - 0s - loss: 238.2466 - mse: 238.2466\n",
      "Epoch 460/500\n",
      "2/2 - 0s - loss: 238.1884 - mse: 238.1884\n",
      "Epoch 461/500\n",
      "2/2 - 0s - loss: 238.1307 - mse: 238.1307\n",
      "Epoch 462/500\n",
      "2/2 - 0s - loss: 238.0730 - mse: 238.0730\n",
      "Epoch 463/500\n",
      "2/2 - 0s - loss: 238.0144 - mse: 238.0144\n",
      "Epoch 464/500\n",
      "2/2 - 0s - loss: 237.9566 - mse: 237.9566\n",
      "Epoch 465/500\n",
      "2/2 - 0s - loss: 237.8978 - mse: 237.8978\n",
      "Epoch 466/500\n",
      "2/2 - 0s - loss: 237.8409 - mse: 237.8409\n",
      "Epoch 467/500\n",
      "2/2 - 0s - loss: 237.7817 - mse: 237.7817\n",
      "Epoch 468/500\n",
      "2/2 - 0s - loss: 237.7242 - mse: 237.7242\n",
      "Epoch 469/500\n",
      "2/2 - 0s - loss: 237.6658 - mse: 237.6658\n",
      "Epoch 470/500\n",
      "2/2 - 0s - loss: 237.6085 - mse: 237.6085\n",
      "Epoch 471/500\n",
      "2/2 - 0s - loss: 237.5494 - mse: 237.5494\n",
      "Epoch 472/500\n",
      "2/2 - 0s - loss: 237.4911 - mse: 237.4911\n",
      "Epoch 473/500\n",
      "2/2 - 0s - loss: 237.4336 - mse: 237.4336\n",
      "Epoch 474/500\n",
      "2/2 - 0s - loss: 237.3755 - mse: 237.3755\n",
      "Epoch 475/500\n",
      "2/2 - 0s - loss: 237.3163 - mse: 237.3163\n",
      "Epoch 476/500\n",
      "2/2 - 0s - loss: 237.2586 - mse: 237.2586\n",
      "Epoch 477/500\n",
      "2/2 - 0s - loss: 237.2008 - mse: 237.2008\n",
      "Epoch 478/500\n",
      "2/2 - 0s - loss: 237.1438 - mse: 237.1438\n",
      "Epoch 479/500\n",
      "2/2 - 0s - loss: 237.0842 - mse: 237.0842\n",
      "Epoch 480/500\n",
      "2/2 - 0s - loss: 237.0268 - mse: 237.0268\n",
      "Epoch 481/500\n",
      "2/2 - 0s - loss: 236.9687 - mse: 236.9687\n",
      "Epoch 482/500\n",
      "2/2 - 0s - loss: 236.9099 - mse: 236.9099\n",
      "Epoch 483/500\n",
      "2/2 - 0s - loss: 236.8532 - mse: 236.8532\n",
      "Epoch 484/500\n",
      "2/2 - 0s - loss: 236.7950 - mse: 236.7950\n",
      "Epoch 485/500\n",
      "2/2 - 0s - loss: 236.7378 - mse: 236.7378\n",
      "Epoch 486/500\n",
      "2/2 - 0s - loss: 236.6781 - mse: 236.6782\n",
      "Epoch 487/500\n",
      "2/2 - 0s - loss: 236.6207 - mse: 236.6207\n",
      "Epoch 488/500\n",
      "2/2 - 0s - loss: 236.5644 - mse: 236.5644\n",
      "Epoch 489/500\n",
      "2/2 - 0s - loss: 236.5043 - mse: 236.5043\n",
      "Epoch 490/500\n",
      "2/2 - 0s - loss: 236.4483 - mse: 236.4483\n",
      "Epoch 491/500\n",
      "2/2 - 0s - loss: 236.3908 - mse: 236.3908\n",
      "Epoch 492/500\n",
      "2/2 - 0s - loss: 236.3321 - mse: 236.3321\n",
      "Epoch 493/500\n",
      "2/2 - 0s - loss: 236.2749 - mse: 236.2749\n",
      "Epoch 494/500\n",
      "2/2 - 0s - loss: 236.2166 - mse: 236.2166\n",
      "Epoch 495/500\n",
      "2/2 - 0s - loss: 236.1594 - mse: 236.1594\n",
      "Epoch 496/500\n",
      "2/2 - 0s - loss: 236.1020 - mse: 236.1020\n",
      "Epoch 497/500\n",
      "2/2 - 0s - loss: 236.0440 - mse: 236.0440\n",
      "Epoch 498/500\n",
      "2/2 - 0s - loss: 235.9877 - mse: 235.9877\n",
      "Epoch 499/500\n",
      "2/2 - 0s - loss: 235.9286 - mse: 235.9286\n",
      "Epoch 500/500\n",
      "2/2 - 0s - loss: 235.8717 - mse: 235.8717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x164813fa0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality **After fit()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>initial_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  initial_pred\n",
       "abbrev                     \n",
       "AL       18.8           0.5\n",
       "AK       18.1           0.5\n",
       "AZ       18.6           0.5\n",
       "AR       22.4           0.5\n",
       "CA       12.0           0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `relu` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tanh` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the predictions changing? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the `Weights`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot (w_1) + alcohol \\cdot (w_2) \\ + ... + \\ ins\\_losses \\cdot (w_7) \\\\\n",
    "accidents = speeding \\cdot (0) + alcohol \\cdot (0) \\ + ... + \\ ins\\_losses \\cdot (0) \\\\\n",
    "accidents = speeding \\cdot (1) + alcohol \\cdot (1) \\ + ... + \\ ins\\_losses \\cdot (1) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to `glorot_uniform` (default)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/optimizers/#available-optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=324\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=324\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers comparison in GIF ‚Üí https://mlfromscratch.com/optimizers-explained/#adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesla's Neural Network Models is composed of 48 models trainned in 70.000 hours of GPU ‚Üí https://tesla.com/ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Year with a 8 GPU Computer ‚Üí https://twitter.com/thirdrowtesla/status/1252723358342377472"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gradient Descent `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `compile()` the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 12:37:42.518624: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-07 12:37:42.522192: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-01-07 12:37:42.615884: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-07 12:37:42.821654: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `ADAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `RMSPROP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it take different times to get the best accuracy? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the end, what should be a feasible configuration of the Neural Network for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `kernel_initializer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `activation` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `optimizer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `loss` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network's importance to find **Non-Linear Patterns** in the Data\n",
    "\n",
    "> - The number of Neurons & Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.87287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Mathematical Formula\n",
    "- Weights / Kernel Initializer\n",
    "- Loss Function\n",
    "- Activation Function\n",
    "- Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What cannot you change arbitrarily of a Neural Network?\n",
    "\n",
    "- Input Neurons\n",
    "- Output Neurons\n",
    "- Loss Functions\n",
    "- Activation Functions"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jes√∫s L√≥pez @sotastica"
   }
  ],
  "interpreter": {
   "hash": "a2b8701b642343483c5f5e717bcb0768c6b951acf38d76a6fc8ea01492bda71d"
  },
  "kernelspec": {
   "display_name": "DeepLearning Python",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
